# -*- coding: utf-8 -*-
"""Entrega04 - Projeto CCF 425.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1F6e_b39lJ795W-ESThZHWUJTAiFkB5az

# **Projeto de CCF 425 - Introdu√ß√£o √† Ci√™ncia de Dados**

##Observa√ß√£o
Mudan√ßas importantes foram realizadas quanto √† organiza√ß√£o desse notebook entre a terceira e quarta entrega.
<p>Os cap√≠tulos que mais foram afetados com as altera√ß√µes s√£o "Perguntas", "Resolu√ß√£o das Perguntas Levantadas no Cap√≠tulo Anterior", "Associa√ß√£o de Dados (Terceira Entrega)" e "Regress√£o Linear (Quarta Entrega)".

## Introdu√ß√£o

Neste projeto, realizaremos um estudo baseado no conjunto de dados BrStats, disponibilizado pelo Prof. Fabr√≠cio e pelo monitor Jo√£o Marcos, al√©m de quatro outros conjuntos de dados selecionados pelo grupo (casos_2020, casos_2021, casos_2022 e casos_2023). O objetivo √© analisar os impactos da pandemia de Covid-19 no Brasil e investigar poss√≠veis rela√ß√µes entre as caracter√≠sticas socioecon√¥micas das cidades e o n√≠vel de impacto da pandemia.

O primeiro conjunto de dados, **"BrStats: A Socioeconomic Statistics Dataset of the Brazilian Cities"**, cont√©m informa√ß√µes estat√≠sticas sobre cidades brasileiras, compiladas a partir de fontes como IBGE, IPEA e DATASUS. Entre os indicadores dispon√≠veis, destacam-se:

- PIB
- Popula√ß√£o
- Receitas municipais
- Dados sobre nascimentos e √≥bitos infantis
- Entre outros fatores que ajudam a compreender o desenvolvimento dessas localidades.

Diante da pandemia global da Covid-19, que impactou todas as regi√µes do mundo, este estudo busca explorar como diferentes cidades brasileiras foram afetadas. Com base nos dados coletados, investigaremos se h√° correla√ß√£o entre as caracter√≠sticas socioecon√¥micas das cidades e a severidade dos impactos causados pela pandemia, contribuindo para uma melhor compreens√£o dos fatores que influenciaram a propaga√ß√£o e os efeitos da doen√ßa no Brasil.

---

## Divis√£o das Tarefas entre os Membros da Equipe

### Entrega 01:
- **Henrique Alves**: Elabora√ß√£o de algumas perguntas
- **Marcos Biscotto**: Elabora√ß√£o de algumas perguntas e An√°lises Iniciais
- **Matheus Nogueira**: Elabora√ß√£o de algumas perguntas
- **Matheus Peixoto**: Obten√ß√£o dos Dados, An√°lises Iniciais e Organiza√ß√£o do GitHub

### Entrega 02:
- **Henrique Alves**: Elabora√ß√£o de alguns gr√°ficos e reorganiza√ß√£o de parte do notebook
- **Marcos Biscotto**: Reorganiza√ß√£o de parte do Notebook conforme as observa√ß√µes passadas √† equipe, Elabora√ß√£o de alguns gr√°ficos e Reformula√ß√£o de Perguntas
- **Matheus Nogueira**: Elabora√ß√£o de alguns gr√°ficos
- **Matheus Peixoto**: Reorganiza√ß√£o de parte do Notebook conforme as observa√ß√µes passadas √† equipe e GitHub

### Entrega 03:
- **Henrique Alves**: An√°lises referente √† terceira entrega
- **Marcos Biscotto**: Melhoria da formata√ß√£o do texto e corre√ß√£o do Markdown, reorganiza√ß√£o da estrutura do notebook, melhoria na legibilidade e refatora√ß√£o de alguns gr√°ficos.
- **Matheus Nogueira**: An√°lises referentes √† terceira entrega
- **Matheus Peixoto**: Organiza√ß√£o do notebook, alguns testes pra an√°lises dessa terceira entrega como infer√™ncias e outras estat√≠sticas

### Entrega 04:
- **Henrique Alves**: Organiza√ß√£o do notebook e an√°lises referentes √† quarta entrega
- **Marcos Biscotto**: Organiza√ß√£o do notebook e an√°lises referentes √† quarta entrega
- **Matheus Nogueira**: An√°lises referentes √† quarta entrega
- **Matheus Peixoto**: Completa revis√£o e reorganiza√ß√£o do notebook entre terceira e quarta entrega; corre√ß√µes quanto ao tratamento de algumas colunas dos datasets extras referentes √† COVID; "An√°lise da Rela√ß√£o entre Popula√ß√£o e N√∫mero de Nascimentos nas Cidades Brasileiras por meio de Regress√£o Linear"


---

## Entendimento Inicial dos Dados

Nessa se√ß√£o iremos importar os dados, buscando realizar uma primeira an√°lise do Dataset. Para tal, utilizaremos principalmente a biblioteca pandas e numpy.

Para o desenvolvimento dos gr√°ficos e demais an√°lises, geopandas, seaborn, matplotlib e algumas outras foram utilizadas.

Executando o c√≥digo a seguir, todas essas bibliotecas ser√£o importadas.
"""

import pandas as pd
import geopandas as gpd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from mlxtend.frequent_patterns import apriori, association_rules
import statsmodels.api as sm
from statsmodels.stats.outliers_influence import variance_inflation_factor
from scipy import stats
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_squared_error
import math

"""##Importa√ß√£o do Arquivo BrStats.csv

Para a realiza√ß√£o do pr√≥ximo passo, importe o arquivo disponibilizado no Cap√≠tulo 4 do artigo "BrStats: a socioeconomic statistics dataset of the Brazilian cities" com o nome "BrStats.csv".

Devido suas caracter√≠sticas, foi preciso passar como par√¢metro a separa√ß√£o utilizando ponto e v√≠rgula, bem como a op√ß√£o low_memory=False, para que o GoogleColab consiga lidar com esse grande volume de dados.
"""

df_br = pd.read_csv('BrStats.csv', sep=";", low_memory=False)
print(df_br.dtypes)
print(f"BrStats: {df_br.shape[0]} linhas, {df_br.shape[1]} colunas")

"""###Extraindo as colunas com v√≠rgula

A partir dessa extra√ß√£o, saberemos quais colunas tratar para melhorar nossa an√°lise dos dados.
"""

colunas_com_virgula = df_br.columns[df_br.apply(lambda col: col.astype(str).str.contains(',', na=False).any())]
print(colunas_com_virgula)

"""###Pr√©-processamento de Dados

**Convers√£o de Tipos de Dados**

Identificamos colunas num√©ricas armazenadas como texto devido a:



*   Uso de v√≠rgulas como separador decimal
*   Pontos como separador de milhares
*   Valores ausentes representados como espa√ßos em branco

Desse modo, implementados a seguinte solu√ß√£o:
"""

colunas_numericas_float = ['Area', 'Importacoes_US$', 'Exportacoes_US$', 'Receitas_R$',
                     'Transferencias_correntes_R$', 'Transferencias_capital_R$', 'povoamento']

colunas_numericas_int = ['Populacao', 'PessoalOcupado', 'PessoalAssalariado', 'VrSalarios',
                     'PIB', 'QtEmpresas','AreaPlantada_h', 'AreaColhida_h', 'VlProducaoAgricola', 'NrNascimentos',
                         'VlProducaoPecuaria', 'NrObitosInfantis']



df_br[colunas_numericas_float] = df_br[colunas_numericas_float].apply(lambda x:
    x.astype(str).str.replace('.', '', regex=False).str.replace(',', '.', regex=False)
    .replace('nan', None).astype(float)
)


df_br[colunas_numericas_int] = df_br[colunas_numericas_int].apply(lambda x:
    x.astype(str).str.replace('.', '', regex=False).replace('nan', '0')
    .fillna(0).astype(int)
)


print(df_br.dtypes)
print(f"BrStats: {df_br.shape[0]} linhas, {df_br.shape[1]} colunas")

"""**Resultado:**

- Colunas num√©ricas convertidas para float ou int
- Valores ausentes padronizados como 0
- Dados prontos para an√°lise quantitativa

Com os dados tratados desta forma, a an√°lise dos mesmos e a obten√ß√£o de informa√ß√µes a partir deles ocorrer√° de forma mais precisa.

---

##Descri√ß√£o do Conjunto de Dados - BrStats

No documento fornecido s√£o informadas as colunas presentes no conjunto de dados. Na tabela abaixo apresentamos cada uma com uma breve descri√ß√£o:

| Coluna                      | Descri√ß√£o                                                                                      |
|----------------------------|------------------------------------------------------------------------------------------------|
| Ano                        | Ano de refer√™ncia dos dados.                                                                   |
| CDMunicipio                | C√≥digo do munic√≠pio conforme a base oficial.                                                   |
| Populacao                  | N√∫mero total de habitantes do munic√≠pio.                                                       |
| PessoalOcupado             | Pessoas empregadas (formais e informais).                                                      |
| PessoalAssalariado         | Pessoas empregadas formalmente (com sal√°rio).                                                  |
| VrSalarios                 | Valor total da massa salarial paga no munic√≠pio.                                               |
| PIB                        | Produto Interno Bruto do munic√≠pio.                                                            |
| QtEmpresas                 | Quantidade total de empresas registradas.                                                      |
| AreaPlantada_h             | √Årea total plantada na agricultura.                                                            |
| AreaColhida_h              | √Årea total colhida na agricultura.                                                             |
| VlProducaoAgricola         | Valor total da produ√ß√£o agr√≠cola.                                                              |
| VlProducaoPecuaria         | Valor total da produ√ß√£o pecu√°ria.                                                              |
| Area                       | √Årea total do munic√≠pio (em km¬≤).                                                              |
| Importacoes_US             | Valor total das importa√ß√µes em d√≥lares americanos.                                                        |
| Exportacoes_US             | Valor total das exporta√ß√µes em d√≥lares americanos.                                                        |
| Receitas_R                 | Receita total do munic√≠pio em reais.                                                           |
| Transferencias_correntes_R| Valor recebido por transfer√™ncias correntes.                                                   |
| Transferencias_capital_R   | Valor recebido por transfer√™ncias de capital.                                                  |
| NrNascimentos              | N√∫mero total de nascimentos registrados.                                                       |
| NrObitosInfantis           | N√∫mero total de √≥bitos infantis registrados.                                                   |
| povoamento                 | Densidade demogr√°fica (pessoas por km¬≤).                                            |
| UF                         | Sigla do estado do munic√≠pio (ex: MG, SP, BA).                                                                  |
| Municipio                  | Nome do munic√≠pio.                                                                             |
| Regiao                     | Regi√£o geogr√°fica do munic√≠pio (Norte, Nordeste, Sul, Sudeste, Centro-Oeste).

Tendo conhecimento dessas informa√ß√µes, realizaremos agora um detalhamento maior dessas colunas, apresentando, para cada uma, seu tipo, quantidade de valores nulos e √∫nicos, e, para aquelas num√©ricas, apresentaremos tamb√©m os valores m√≠nimos e m√°ximos encontrados, bem como a m√©dia e a mediana.
"""

desc_numericas = df_br.describe().T

nulos = df_br.isnull().sum()

tipos = df_br.dtypes

valores_unicos = df_br.nunique()

resumo = pd.DataFrame({
    'Tipo': tipos,
    'Nulos': nulos,
    'Valores √∫nicos': valores_unicos,
    'M√≠nimo': desc_numericas['min'],
    'M√°ximo': desc_numericas['max'],
    'M√©dia': desc_numericas['mean'],
    'Mediana': df_br.median(numeric_only=True)
})

resumo = resumo.fillna("-")

resumo

"""---

##Gr√°ficos a partir do Conjunto de Dados - BrStats

###Dados gerais sobre o Conjunto de Dados  - BrStats

## 1. Densidade Demogr√°fica M√©dia por Regi√£o
<p>Mostra a m√©dia da densidade populacional (hab/km¬≤) dos munic√≠pios agrupados por regi√£o do Brasil. √â poss√≠vel observar que regi√µes como Sudeste e Sul apresentam maior concentra√ß√£o populacional, enquanto Norte e Centro-Oeste t√™m menor densidade, refletindo diferen√ßas no processo de ocupa√ß√£o do territ√≥rio.
"""

povoamento_por_regiao = df_br.groupby('Regiao')['povoamento'].mean().reset_index()
estados = gpd.read_file('https://raw.githubusercontent.com/codeforamerica/click_that_hood/master/public/data/brazil-states.geojson')

regioes = {
    'Norte': ['Acre', 'Amap√°', 'Amazonas', 'Par√°', 'Rond√¥nia', 'Roraima', 'Tocantins'],
    'Nordeste': ['Alagoas', 'Bahia', 'Cear√°', 'Maranh√£o', 'Para√≠ba', 'Pernambuco', 'Piau√≠', 'Rio Grande do Norte', 'Sergipe'],
    'Centro-Oeste': ['Distrito Federal', 'Goi√°s', 'Mato Grosso', 'Mato Grosso do Sul'],
    'Sudeste': ['Esp√≠rito Santo', 'Minas Gerais', 'Rio de Janeiro', 'S√£o Paulo'],
    'Sul': ['Paran√°', 'Rio Grande do Sul', 'Santa Catarina']
}

def estado_para_regiao(nome_estado):
    for regiao, lista_estados in regioes.items():
        if nome_estado in lista_estados:
            return regiao
    return None

estados['Regiao'] = estados['name'].apply(estado_para_regiao)

regioes_geo = estados.dissolve(by='Regiao', as_index=False)

regioes_geo = regioes_geo.merge(povoamento_por_regiao, on='Regiao')

fig, ax = plt.subplots(figsize=(10, 8))
regioes_geo.plot(column='povoamento', cmap='YlOrRd', edgecolor='black', linewidth=0.8, legend=True, ax=ax)

for idx, row in regioes_geo.iterrows():
    plt.annotate(
        text=f"{row['povoamento']:.1f}",
        xy=row['geometry'].centroid.coords[0],
        ha='center',
        fontsize=9,
        weight='bold'
    )

plt.title('Densidade Demogr√°fica M√©dia por Regi√£o (hab/km¬≤)', fontsize=14)
plt.axis('off')
plt.tight_layout()
plt.show()

"""√â poss√≠vel observar que regi√µes como Sudeste e Sul apresentam maior concentra√ß√£o populacional, enquanto Norte e Centro-Oeste t√™m menor densidade, refletindo diferen√ßas no processo de ocupa√ß√£o do territ√≥rio.

###2. Popula√ß√£o vs QtEmpresas (por ano)
<p> Evidencia a concentra√ß√£o e distribui√ß√£o da quantidade de empresas e a popula√ß√£o de cada munic√≠pio com o passar dos anos (2016 - 2021), mostrando a quantidade de cidades est√£o presentes em cada par Quantidade de Empresas X Popula√ß√£o.
"""

palette_regioes = {
    'Norte': '#1f77b4',       # azul
    'Nordeste': '#ff7f0e',    # laranja
    'Centro-Oeste': '#2ca02c',# verde
    'Sudeste': '#d62728',     # vermelho
    'Sul': '#9467bd'          # roxo
}
anos = sorted(df_br['Ano'].unique())

# Definir faixas de popula√ß√£o
bins_pop = [0, 10000, 100000, 1000000, 13000000]
labels_pop = ['0-10 mil', '10k-100k', '100k-1M', '1M-13M']

# Definir faixas de quantidade de empresas
bins_emp = [0, 132611, 265223, 530446]
labels_emp = ['0-132k', '132k-265k', '265k-530k']

n_cols = 2
n_rows = math.ceil(len(anos) / n_cols)

fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols * 6, n_rows * 5))

# Se axes for 1D, transformar em 2D
axes = axes.reshape(n_rows, n_cols)

for idx, ano in enumerate(anos):
    row = idx // n_cols
    col = idx % n_cols

    ax = axes[row, col]

    df_ano = df_br[df_br['Ano'] == ano].copy()

    df_ano['Faixa_Pop'] = pd.cut(df_ano['Populacao'], bins=bins_pop, labels=labels_pop, include_lowest=True)
    df_ano['Faixa_Emp'] = pd.cut(df_ano['QtEmpresas'], bins=bins_emp, labels=labels_emp, include_lowest=True)

    heatmap_data = pd.crosstab(df_ano['Faixa_Emp'], df_ano['Faixa_Pop'])

    sns.heatmap(
        heatmap_data,
        annot=True,
        fmt='d',
        cmap='YlGnBu',
        ax=ax
    )

    ax.set_title(f'Heatmap Popula√ß√£o vs Empresas ({ano})')
    ax.set_xlabel('Faixa de Popula√ß√£o')
    ax.set_ylabel('Faixa de Empresas')

# Remover plots vazios se houver
total_plots = n_rows * n_cols
if total_plots > len(anos):
    for idx in range(len(anos), total_plots):
        row = idx // n_cols
        col = idx % n_cols
        fig.delaxes(axes[row, col])

plt.tight_layout()
plt.subplots_adjust(hspace=0.4, wspace=0.3)
plt.show()

"""<p> √â poss√≠vel notar a sutil diferen√ßa entre cada um dos anos, indicando que mesmo que pouco, as cidades apresentaram um comportamento diferente nesse quesito.

###3. PIB m√©dio por Regi√£o ao longo dos anos
<p> Relaciona a m√©dia do PIB das cinco regi√µes brasileiras ao longo dos anos (2016 - 2020)

N√£o h√° valores para o PIB de 2021, ent√£o removemos o ano do gr√°fico.
"""

plt.figure(figsize=(10,6))
pib_por_regiao = df_br.groupby(['Ano', 'Regiao'])['PIB'].mean().reset_index()
pib_por_regiao = pib_por_regiao[pib_por_regiao['Ano'] != 2021]
sns.lineplot(data=pib_por_regiao, x='Ano', y='PIB', hue='Regiao', marker='o')
plt.title('PIB m√©dio por Regi√£o ao longo dos anos (sem 2021)')
plt.ylabel('PIB m√©dio (R$)')
plt.xlabel('Ano')
plt.grid(True)
plt.legend(loc='upper right', bbox_to_anchor=(1.15, 1.0))
plt.tight_layout()
plt.show()

"""###4. Rela√ß√£o entre Sal√°rios e Pessoal Assalariado
<p> Evidencia a concentra√ß√£o e distribui√ß√£o da faixa salarial e a popula√ß√£o assalariada de cada munic√≠pio com o passar dos anos (2016 - 2021), mostrando a quantidade de cidades est√£o presentes em cada par Valores de Sal√°rios X Popula√ß√£o Assalariada.
<p> √â poss√≠vel notar a sutil diferen√ßa entre cada um dos anos, indicando que mesmo que pouco, as cidades apresentaram um comportamento diferente nesse quesito.
"""

palette_regioes = {
    'Norte': '#1f77b4',       # azul
    'Nordeste': '#ff7f0e',    # laranja
    'Centro-Oeste': '#2ca02c',# verde
    'Sudeste': '#d62728',     # vermelho
    'Sul': '#9467bd'          # roxo
}

# Definir faixas de Pessoal Assalariado
bins_assalariado = [0, 100_000, 1_000_000, 3_000_000, 6_000_000]
labels_assalariado = ['0-100k', '100k-1M', '1M-3M', '3M-6M']

# Definir faixas de Valor dos Sal√°rios
bins_salarios = [0, 10_000, 10_000_000, 100_000_000, 300_000_000]
labels_salarios = ['0-10k', '10k-10M', '10M-100M', '100M-300M']

# Preparar subplot com 2 colunas e N linhas
n_anos = len(anos)
n_cols = 2
n_rows = (n_anos + 1) // 2  # arredonda pra cima

fig, axes = plt.subplots(n_rows, n_cols, figsize=(14, n_rows * 5))

axes = axes.flatten()

for idx, ano in enumerate(anos):
    ax = axes[idx]

    df_ano = df_br[df_br['Ano'] == ano].copy()

    df_ano['Faixa_Assalariado'] = pd.cut(
        df_ano['PessoalAssalariado'],
        bins=bins_assalariado,
        labels=labels_assalariado,
        include_lowest=True
    )

    df_ano['Faixa_Salarios'] = pd.cut(
        df_ano['VrSalarios'],
        bins=bins_salarios,
        labels=labels_salarios,
        include_lowest=True
    )

    heatmap_data = pd.crosstab(
        df_ano['Faixa_Salarios'],
        df_ano['Faixa_Assalariado']
    )

    sns.heatmap(
        heatmap_data,
        annot=True,
        fmt='d',
        cmap='YlOrRd',
        ax=ax
    )

    ax.set_title(f'Heatmap Sal√°rios vs Pessoal Assalariado ({ano})')
    ax.set_xlabel('Faixa de Pessoal Assalariado')
    ax.set_ylabel('Faixa de Valor dos Sal√°rios')

# Esconde os gr√°ficos vazios se o n√∫mero de anos for √≠mpar
for j in range(idx + 1, len(axes)):
    axes[j].set_visible(False)

plt.tight_layout()
plt.subplots_adjust(hspace=0.4, wspace=0.3)  # Espa√ßamento entre os gr√°ficos
plt.show()

""" ### 5. √Årea Plantada vs. √Årea Colhida
 <p> Evidencia a concentra√ß√£o e distribui√ß√£o da √°rea plantada e a colhida de cada munic√≠pio com o passar dos anos (2016 - 2021), mostrando a quantidade de cidades est√£o presentes em cada par √Årea Plantada X √Årea Colhida.
 <p> √â poss√≠vel notar a sutil diferen√ßa entre cada um dos anos, indicando que mesmo que pouco, as cidades apresentaram um comportamento diferente nesse quesito.
"""

palette_regioes = {
    'Norte': '#1f77b4',       # azul
    'Nordeste': '#ff7f0e',    # laranja
    'Centro-Oeste': '#2ca02c',# verde
    'Sudeste': '#d62728',     # vermelho
    'Sul': '#9467bd'          # roxo
}
# Definir faixas para √Årea Plantada
bins_plantada = [0, 2500, 5000, 7500, 10000]
labels_plantada = ['0-2.5k', '2.5k-5k', '5k-7.5k', '7.5k-10k']

# Definir faixas para √Årea Colhida
bins_colhida = [0, 2500, 5000, 7500, 10000]
labels_colhida = ['0-2.5k', '2.5k-5k', '5k-7.5k', '7.5k-10k']

# Configura√ß√£o dos subplots
n_anos = len(anos)
n_cols = 2
n_rows = (n_anos + 1) // 2  # arredonda pra cima

fig, axes = plt.subplots(n_rows, n_cols, figsize=(14, n_rows * 5))

axes = axes.flatten()

for idx, ano in enumerate(anos):
    ax = axes[idx]

    df_ano = df_br[df_br['Ano'] == ano].copy()

    # Categorizar os dados nas faixas
    df_ano['Faixa_Plantada'] = pd.cut(
        df_ano['AreaPlantada_h'],
        bins=bins_plantada,
        labels=labels_plantada,
        include_lowest=True
    )

    df_ano['Faixa_Colhida'] = pd.cut(
        df_ano['AreaColhida_h'],
        bins=bins_colhida,
        labels=labels_colhida,
        include_lowest=True
    )

    # Gerar tabela cruzada
    heatmap_data = pd.crosstab(
        df_ano['Faixa_Colhida'],
        df_ano['Faixa_Plantada']
    )

    # Plotar heatmap
    sns.heatmap(
        heatmap_data,
        annot=True,
        fmt='d',
        cmap='BuGn',
        ax=ax
    )

    ax.set_title(f'Heatmap √Årea Plantada vs √Årea Colhida ({ano})')
    ax.set_xlabel('Faixa de √Årea Plantada (ha)')
    ax.set_ylabel('Faixa de √Årea Colhida (ha)')

# Esconde gr√°ficos vazios (se houver)
for j in range(idx + 1, len(axes)):
    axes[j].set_visible(False)

plt.tight_layout()
plt.subplots_adjust(hspace=0.4, wspace=0.3)  # Espa√ßamento entre linhas e colunas
plt.show()

"""###6. Produ√ß√£o Agr√≠cola vs. Pecu√°ria
<p> Evidencia a concentra√ß√£o e distribui√ß√£o da produ√ß√£o pecu√°ria e a produ√ß√£o agr√≠cola de cada munic√≠pio com o passar dos anos (2016 - 2021), mostrando a quantidade de cidades est√£o presentes em cada par Produ√ß√£o Pecu√°ria X Produ√ß√£o Agr√≠cola.
<p> √â poss√≠vel notar a sutil diferen√ßa entre cada um dos anos, indicando que mesmo que pouco, as cidades apresentaram um comportamento diferente nesse quesito.
"""

palette_regioes = {
    'Norte': '#1f77b4',       # azul
    'Nordeste': '#ff7f0e',    # laranja
    'Centro-Oeste': '#2ca02c',# verde
    'Sudeste': '#d62728',     # vermelho
    'Sul': '#9467bd'          # roxo
}
# Definir faixas para Produ√ß√£o Agr√≠cola
bins_agricola = [0, 500, 3000, 7000, 10000]
labels_agricola = ['0-500', '500-3k', '3k-7k', '7k-10k']

# Definir faixas para Produ√ß√£o Pecu√°ria
bins_pecuaria = [0, 500, 3000, 7000, 10000]
labels_pecuaria = ['0-500', '500-3k', '3k-7k', '7k-10k']

# Configura√ß√£o dos subplots
n_anos = len(anos)
n_cols = 2
n_rows = (n_anos + 1) // 2  # arredonda pra cima

fig, axes = plt.subplots(n_rows, n_cols, figsize=(14, n_rows * 5))
axes = axes.flatten()

for idx, ano in enumerate(anos):
    ax = axes[idx]

    df_ano = df_br[df_br['Ano'] == ano].copy()

    # Categorizar os dados nas faixas
    df_ano['Faixa_Agricola'] = pd.cut(
        df_ano['VlProducaoAgricola'],
        bins=bins_agricola,
        labels=labels_agricola,
        include_lowest=True
    )

    df_ano['Faixa_Pecuaria'] = pd.cut(
        df_ano['VlProducaoPecuaria'],
        bins=bins_pecuaria,
        labels=labels_pecuaria,
        include_lowest=True
    )

    # Gerar tabela cruzada
    heatmap_data = pd.crosstab(
        df_ano['Faixa_Pecuaria'],
        df_ano['Faixa_Agricola']
    )

    # Plotar heatmap
    sns.heatmap(
        heatmap_data,
        annot=True,
        fmt='d',
        cmap='YlOrRd',
        ax=ax
    )

    ax.set_title(f'Heatmap Produ√ß√£o Agr√≠cola vs. Pecu√°ria ({ano})')
    ax.set_xlabel('Faixa de Produ√ß√£o Agr√≠cola (R$)')
    ax.set_ylabel('Faixa de Produ√ß√£o Pecu√°ria (R$)')

# Esconde gr√°ficos vazios (se houver)
for j in range(idx + 1, len(axes)):
    axes[j].set_visible(False)

plt.tight_layout()
plt.subplots_adjust(hspace=0.4, wspace=0.3)
plt.show()

"""###7. Exporta√ß√µes e Importa√ß√µes por Regi√£o (m√©dia anual)
<p> Faz uma m√©dia das importa√ß√µes e importa√ß√µes anuais para cada uma das cinco regi√µes do pa√≠s.
"""

exp_imp = df_br.groupby('Regiao')[['Exportacoes_US$', 'Importacoes_US$']].mean().reset_index()
exp_imp.plot(x='Regiao', kind='bar', figsize=(10,6))
plt.title('Exporta√ß√µes e Importa√ß√µes M√©dias por Regi√£o')
plt.ylabel('Valor (US$)')
plt.xlabel('Regi√£o')
plt.legend(loc='upper right', bbox_to_anchor=(1.15, 1.0))
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

"""###8. Nascimentos vs. √ìbitos Infantis
<p> Evidencia a concentra√ß√£o e distribui√ß√£o do n√∫mero de √≥bitos infantis e a taxa de natalidade de cada munic√≠pio com o passar dos anos (2016 - 2021), mostrando a quantidade de cidades est√£o presentes em cada par √ìbitos Infantis X Natalidade.
<p> √â poss√≠vel notar a sutil diferen√ßa entre cada um dos anos, indicando que mesmo que pouco, os munic√≠pios apresentaram um comportamento diferente nesse quesito.
"""

palette_regioes = {
    'Norte': '#1f77b4',       # azul
    'Nordeste': '#ff7f0e',    # laranja
    'Centro-Oeste': '#2ca02c',# verde
    'Sudeste': '#d62728',     # vermelho
    'Sul': '#9467bd'          # roxo
}
anos = sorted(df_br['Ano'].unique())

# Configura√ß√£o dos subplots
n_anos = len(anos)
n_cols = 2
n_rows = (n_anos + 1) // 2  # arredonda pra cima

fig, axes = plt.subplots(n_rows, n_cols, figsize=(14, n_rows * 5))
axes = axes.flatten()

for idx, ano in enumerate(anos):
    ax = axes[idx]
    df_ano = df_br[df_br['Ano'] == ano]

    hb = ax.hexbin(
        x=df_ano['NrNascimentos'],
        y=df_ano['NrObitosInfantis'],
        gridsize=30,
        cmap='YlOrRd',
        mincnt=1
    )

    ax.set_title(f'Nascimentos vs. √ìbitos Infantis - {ano}')
    ax.set_xlabel('Nascimentos')
    ax.set_ylabel('√ìbitos Infantis')

    cb = fig.colorbar(hb, ax=ax)
    cb.set_label('Quantidade de Munic√≠pios')

# Esconde gr√°ficos vazios (se sobrar espa√ßo na grid)
for j in range(idx + 1, len(axes)):
    axes[j].set_visible(False)

plt.tight_layout()
plt.subplots_adjust(hspace=0.4, wspace=0.3)
plt.show()

"""---

##Dados relacionados √† pandemia do Covid-19 - BrStats

###1. Queda no n√∫mero de empresas
Objetivo: Ver se houve redu√ß√£o no n√∫mero de empresas ativas.

Gr√°fico de linha com o eixo X sendo o ano e Y sendo a QtEmpresas.

Uma linha para cada regi√£o.
"""

df_emp = df_br[df_br['Ano'].between(2019, 2021)]
df_group = df_emp.groupby(['Ano', 'Regiao'])['QtEmpresas'].sum().reset_index()
plt.figure(figsize=(10, 6))
sns.lineplot(data=df_group, x='Ano', y='QtEmpresas', hue='Regiao', marker='o')
plt.title('Evolu√ß√£o da Quantidade de Empresas por Regi√£o (2019‚Äì2021)')
plt.ylabel('Quantidade de Empresas')
plt.grid(True)
plt.legend(loc='upper right', bbox_to_anchor=(1.15, 1.0))
plt.tight_layout()
plt.show()

"""###2. PIB por regi√£o ao longo dos anos
Objetivo: Analisar o impacto direto no Produto Interno Bruto.

Gr√°fico de linha do PIB por regi√£o, 2019‚Äì2020.

N√£o h√° valores para o PIB no ano de 2021, ent√£o removemos o ano do gr√°fico.

"""

df_pib = df_br[df_br['Ano'].between(2019, 2021)]
df_group = df_pib.groupby(['Ano', 'Regiao'])['PIB'].sum().reset_index()
df_group = df_group[df_group['Ano'] != 2021]
plt.figure(figsize=(10, 6))
sns.lineplot(data=df_group, x='Ano', y='PIB', hue='Regiao', marker='o')
plt.title('PIB Total por Regi√£o (2019‚Äì2020)')
plt.ylabel('PIB')
plt.xlabel('Ano')
plt.grid(True)
plt.legend(loc='upper right', bbox_to_anchor=(1.15, 1.0))
plt.tight_layout()
plt.show()

"""###3. Impacto no emprego
Objetivo: Ver efeitos no mercado de trabalho.

Gr√°fico de barras para:

*   PessoalOcupado
*   PessoalAssalariado
"""

df_job = df_br[df_br['Ano'].between(2019, 2021)]
df_group = df_job.groupby('Ano')[['PessoalOcupado', 'PessoalAssalariado']].sum().reset_index()
df_group.plot(x='Ano', kind='bar', figsize=(10, 6))
plt.title('Empregos Totais x Assalariados (2019‚Äì2021)')
plt.ylabel('Quantidade de Pessoas')
plt.legend(loc='upper right', bbox_to_anchor=(1.15, 1.0))
plt.xticks(rotation=0)
plt.tight_layout()
plt.show()

"""###4. Nascimentos e √ìbitos Infantis
Objetivo: Investigar impacto demogr√°fico e no sistema de sa√∫de.

Gr√°fico de linha para NrNascimentos e NrObitosInfantis por ano e regi√£o.
"""

from matplotlib.ticker import FuncFormatter
def normal_notation(x, pos):
    return f'{x:.0f}'
df_nasc = df_br[df_br['Ano'].between(2019, 2021)]
df_group = df_nasc.groupby(['Ano', 'Regiao'])[['NrNascimentos', 'NrObitosInfantis']].sum().reset_index()
plt.figure(figsize=(14, 6))
plt.subplot(1, 2, 1)
sns.lineplot(data=df_group, x='Ano', y='NrNascimentos', hue='Regiao', marker='o')
plt.title('Nascimentos por Regi√£o (2019‚Äì2021)')
plt.ylabel('Nascimentos')
plt.xlabel('Ano')
plt.grid(True)
plt.gca().yaxis.set_major_formatter(FuncFormatter(normal_notation))
plt.subplot(1, 2, 2)
sns.lineplot(data=df_group, x='Ano', y='NrObitosInfantis', hue='Regiao', marker='o')
plt.title('√ìbitos Infantis por Regi√£o (2019‚Äì2021)')
plt.ylabel('√ìbitos Infantis')
plt.xlabel('Ano')
plt.grid(True)
plt.ylim(20000, 160000)
plt.gca().yaxis.set_major_formatter(FuncFormatter(normal_notation))
plt.tight_layout()
plt.show()

"""###5. Scatterplots comparando antes e depois de PIB vs QtEmpresas para 2019 vs 2020
Objetivo: Avaliar mudan√ßas em rela√ß√µes econ√¥micas para todos os munic√≠pios de todas as regi√µes.
<p> √â poss√≠vel notar a sutil diferen√ßa entre cada um dos anos, indicando que mesmo que pouco, as regi√µes apresentaram um comportamento diferente nesse quesito.
"""

palette_regioes = {
    'Norte': '#1f77b4',       # azul
    'Nordeste': '#ff7f0e',    # laranja
    'Centro-Oeste': '#2ca02c',# verde
    'Sudeste': '#d62728',     # vermelho
    'Sul': '#9467bd'          # roxo
}
df_filtered = df_br[df_br['Ano'].isin([2019, 2020])]

plt.figure(figsize=(14, 6))

# Gr√°fico para 2019
plt.subplot(1, 2, 1)
sns.scatterplot(data=df_filtered[df_filtered['Ano'] == 2019], x='QtEmpresas', y='PIB', hue='Regiao',
                palette=palette_regioes, alpha=0.7)
plt.title('Rela√ß√£o entre N√∫mero de Empresas e PIB (2019)')
plt.xlabel('Quantidade de Empresas')
plt.ylabel('PIB')
plt.grid(True)
plt.legend(title='Regi√£o', bbox_to_anchor=(1.05, 1), loc='upper left')

# Gr√°fico para 2020
plt.subplot(1, 2, 2)
sns.scatterplot(data=df_filtered[df_filtered['Ano'] == 2020], x='QtEmpresas', y='PIB', hue='Regiao',
                palette=palette_regioes, alpha=0.7)
plt.title('Rela√ß√£o entre N√∫mero de Empresas e PIB (2020)')
plt.xlabel('Quantidade de Empresas')
plt.ylabel('PIB')
plt.grid(True)
plt.legend(title='Regi√£o', bbox_to_anchor=(1.05, 1), loc='upper left')

plt.tight_layout()
plt.show()

"""###6. Boxplot da raz√£o entre Receita e Transfer√™ncia
Objetivo: Identificar autossufici√™ncia fiscal antes e durante a pandemia.

Raz√£o: Receitas_R$ / (Transferencias_correntes_R$ + Transferencias_capital_R$)

Boxplot por regi√£o, em 2019 vs 2020/2021
"""

df_ratio = df_br.copy()
df_ratio['ReceitaPropria'] = df_ratio['Receitas_R$'] - df_ratio['Transferencias_correntes_R$'] - df_ratio['Transferencias_capital_R$']
df_ratio = df_ratio[df_ratio['Ano'].between(2019, 2021)]
df_ratio['Razao'] = df_ratio['ReceitaPropria'] / (df_ratio['Receitas_R$'] + 1)
plt.figure(figsize=(10, 6))
sns.boxplot(data=df_ratio, x='Regiao', y='Razao')
plt.title('Autossufici√™ncia Fiscal por Regi√£o (2019‚Äì2021)')
plt.ylabel('Receita Pr√≥pria / Receita Total')
plt.xticks(rotation=45)
plt.grid(True)
plt.tight_layout()
plt.show()

"""---

---

## Importa√ß√£o dos Dados sobre a COVID-19 - DataSUS

<p>Nessa subse√ß√£o faremos a importa√ß√£o e tamb√©m compreen√ß√£o dos dados que obtivemos sobre a pandemia de COVID 19 que afetou o Brasil e o mundo entre 2020 e 2023.
<p>O primeiro caso da doen√ßa no Brasil foi registrada no dia 25 de Fevereiro de 2020, na cidade de S√£o Paulo. Desde ent√£o, o Minist√©rio da Sa√∫de realizou o monitoramento di√°rio do avan√ßo da pandemia em terras tupiniquins.
<p>Utilizando por base o Minist√©rio da Sa√∫de, o Brasil.IO e o perfil @coronavirusbra1, este levantamento realizado pelo Departamento de F√≠sica da Universidade Federal de Vi√ßosa, liderado pelo Professor Wesley Cota, apresenta dados desde a 9¬™ at√© a 311¬™ semana da pandemia no Brasil.
<p>A seguir faremos a importa√ß√£o desses dados, nomeados como 'casos_2020.csv', 'casos_2021.csv', 'casos_2022.csv' e 'casos_2023.csv'.
"""

df_2020 = pd.read_csv('casos_2020.csv', low_memory=False)
print(f"Casos 2020: {df_2020.shape[0]} linhas, {df_2020.shape[1]} colunas")

df_2021 = pd.read_csv('casos_2021.csv', low_memory=False)
print(f"Casos 2021: {df_2021.shape[0]} linhas, {df_2021.shape[1]} colunas")

df_2022 = pd.read_csv('casos_2022.csv', low_memory=False)
print(f"Casos 2022: {df_2022.shape[0]} linhas, {df_2022.shape[1]} colunas")

df_2023 = pd.read_csv('casos_2023.csv', low_memory=False)
print(f"Casos 2023: {df_2023.shape[0]} linhas, {df_2023.shape[1]} colunas")

"""### Descri√ß√£o dos Dados - casos_2020 a 2023

Como √© poss√≠vel observar pela execu√ß√£o dos trechos de c√≥digo que antecedem essa subse√ß√£o, os c√≥digos foram formatados como esperado, n√£o necessitando uma convers√£o como para o DataSet anterior.

| Coluna                         | Descri√ß√£o                                                                                                          |
|--------------------------------|--------------------------------------------------------------------------------------------------------------------|
| date                           | Data do registro, no formato YYYY-MM-DD.                                                                           |
| country                        | Pa√≠s dos dados, que √© sempre "Brazil".                                                                             |
| state                          | Sigla da unidade federativa (UF) ou "TOTAL" quando se refere ao pa√≠s inteiro.                                      |
| city                           | Nome completo do munic√≠pio no formato "Cidade/UF". Pode ser "CASO SEM LOCALIZACAO DEFINIDA/UF".                  |
| ibgeID                         | ID √∫nico do munic√≠pio fornecido pelo Instituto Brasileiro de Geografia e Estat√≠stica (IBGE).                      |
| newDeaths                      | Diferen√ßa entre o n√∫mero de √≥bitos da data correspondente e o dia anterior.                                        |
| deaths                         | N√∫mero acumulado de √≥bitos at√© aquela data.                                                                        |
| newCases                       | Diferen√ßa entre o n√∫mero de casos da data correspondente e o dia anterior.                                         |
| totalCases                     | N√∫mero acumulado de casos at√© aquela data.                                                                         |
| deaths_per_100k_inhabitants   | N√∫mero de √≥bitos por 100.000 habitantes naquela localidade.                                                        |
| totalCases_per_100k_inhabitants | N√∫mero de casos por 100.000 habitantes naquela localidade.                                                        |
| deaths_by_totalCases           | Propor√ß√£o entre o n√∫mero de √≥bitos e o n√∫mero total de casos.                                                      |

---
<h2>Resumo dos Dados</h2>
Nessa parte, utilizando-se dos c√≥digos a seguir, podemos ter um resumo dos dados dos datasets relacionados √† COVID-19 separados por ano (2020, 2021, 2022 e 2023).
"""

def gerar_resumo(df):
    desc_numericas = df.describe().T
    nulos = df.isnull().sum()
    tipos = df.dtypes
    valores_unicos = df.nunique()

    resumo = pd.DataFrame({
        'Tipo': tipos,
        'Nulos': nulos,
        'Valores √∫nicos': valores_unicos,
        'M√≠nimo': desc_numericas['min'],
        'M√°ximo': desc_numericas['max'],
        'M√©dia': desc_numericas['mean'],
        'Mediana': df.median(numeric_only=True)
    })

    resumo = resumo.fillna("-")
    return resumo

resumo_2020 = gerar_resumo(df_2020)
print(resumo_2020.to_markdown())

resumo_2021 = gerar_resumo(df_2021)
print(resumo_2021.to_markdown())

resumo_2022 = gerar_resumo(df_2022)
print(resumo_2022.to_markdown())

resumo_2023 = gerar_resumo(df_2023)
print(resumo_2023.to_markdown())

"""---

### Cria√ß√£o de Novos Conjuntos de Dados

<p>Visando ter apenas os valores totais da pandemia para cada ano, realizamos uma limpeza dos dados, criando novos conjuntos de dados com estes.
<p>Esses novos DataSets ser√£o utilizados nas an√°lises ao longo do projeto.
<p>Como os dados s√£o cumulativos, o c√≥digo obt√©m o maior n√∫mero relacionado a cada cidade e, em sequ√™ncia, cria o conjunto 'df_ano_max.csv'.
"""

# Filtragem das cidades n√£o nulas
df_2020 = df_2020[df_2020['city'].notna()]
df_2021 = df_2021[df_2021['city'].notna()]
df_2022 = df_2022[df_2022['city'].notna()]
df_2023 = df_2023[df_2023['city'].notna()]

def process_dataframe(df):
    # Agrupar e obter os m√°ximos
    df_max = df.groupby('city')[['state', 'deaths', 'totalCases']].max().reset_index()

    # Corre√ß√£o do c√°lculo de deaths_by_totalCases - estava utilizando valores equivocados anteriormente
    df_max['deaths_by_totalCases'] = (df_max['deaths'] / df_max['totalCases']).round(5)

    return df_max

df_2020_max = process_dataframe(df_2020)
df_2021_max = process_dataframe(df_2021)
df_2022_max = process_dataframe(df_2022)
df_2023_max = process_dataframe(df_2023)

df_2020_max.to_csv('df_2020_max.csv', index=False)
df_2021_max.to_csv('df_2021_max.csv', index=False)
df_2022_max.to_csv('df_2022_max.csv', index=False)
df_2023_max.to_csv('df_2023_max.csv', index=False)

"""Ap√≥s esse processo, criamos com o c√≥digo abaixo uma jun√ß√£o de todos esses datasets em um s√≥, contendo as principais informa√ß√µes de maneira ordenada por cidade e ano, de modo a facilitar visualiza√ß√£o e futuras an√°lises."""

df_2020_max['ano'] = 2020
df_2021_max['ano'] = 2021
df_2022_max['ano'] = 2022
df_2023_max['ano'] = 2023

# Junta todos os DataFrames em um s√≥
df_todos_max = pd.concat([df_2020_max, df_2021_max, df_2022_max, df_2023_max], ignore_index=True)

# Ordena por nome da cidade e ano
df_todos_max = df_todos_max.sort_values(by=['city', 'ano']).reset_index(drop=True)
df_todos_max = df_todos_max[['ano', 'city', 'state', 'deaths', 'totalCases', 'deaths_by_totalCases']]

df_todos_max.to_csv('df_todos_max.csv', index=False)

"""---

---

## Perguntas

<p>Nessa se√ß√£o apresentaremos algumas perguntas que foram elaboradas pelo grupo e ser√£o analisadas no decorrer do Projeto.
<p>Como explicitado na introdu√ß√£o, algumas dessas perguntas tentar√£o responder e encontrar uma correla√ß√£o entre as caracter√≠sticas das cidades e o impacto que a pandemia de COVID-19 gerou nestas.
<p>Al√©m das perguntas voltadas a esta tem√°tica, algumas outras ser√£o apresentadas, buscando entender o cen√°rio do Brasil, como alguns aspectos econ√¥micos, por exemplo. Isto poder√° ser √∫til para entender, mais ao final do projeto, como foi o impacto da pandemia na pa√≠s.


1.   Quais cidades registraram os maiores n√∫meros absolutos de √≥bitos infantis e as maiores raz√µes de √≥bito/nascimento infantil durante a pandemia (2020-2021)?

2.   Qual foi a varia√ß√£o no n√∫mero total de empresas entre o per√≠odo pr√©-pandemia (2018-2019) e o per√≠odo pand√™mico (2020-2021)? Que conclus√µes podemos extrair dessa an√°lise?

3.   Qual a soma dos maiores valores de produ√ß√£o agr√≠cola por cidade no per√≠odo pr√©-pand√™mico (2016-2019) comparado ao per√≠odo pand√™mico (2020-2021)?

4.  Como o crescimento do PIB dos munic√≠pios brasileiros entre 2018 e 2019 se comportou em rela√ß√£o a 2019 e 2020, in√≠cio do per√≠odo pand√™mico?

5.   Existe correla√ß√£o entre receitas municipais e indicadores de qualidade de vida, como mortalidade infantil?

6.   H√° rela√ß√£o direta entre √°rea plantada/colhida e valor total da produ√ß√£o agr√≠cola?

7.   Munic√≠pios com maior volume de exporta√ß√µes apresentam melhores indicadores econ√¥micos?

8.   Qual foi o impacto da pandemia nos fluxos de importa√ß√£o e exporta√ß√£o por regi√£o?

<h3>Observa√ß√£o sobre as Perguntas:</h3>
No decorrer do projeto, algumas novas perguntas foram surgindo e, desse modo, algumas que anteriormente estavam listadas acima foram removidas. Todas as que permaceram acima est√£o sendo respondidas no cap√≠tulo seguinte.

<h3>Outras Perguntas:</h3>
Nos cap√≠tulos seguintes, referentes √† Terceira, Quarta e, futuramente, Quinta Entrega est√£o listadas algumas perguntas que ser√£o respondidas especificamente buscando utilzar os conhecimentos adquiridos sobre Associa√ß√£o de Dados, Regress√£o Linear e Aprendizado Supervisionado.

---

##Resolu√ß√£o das Perguntas Levantadas no Cap√≠tulo Anterior

###Pergunta 01
Quais cidades registraram os maiores n√∫meros absolutos de √≥bitos infantis e as maiores raz√µes de √≥bito/nascimento infantil durante a pandemia (2020-2021)?
"""

df_pandemia = df_br[df_br['Ano'].isin([2020, 2021])].copy()

# Agrupando por munic√≠pio e somando os valores dos dois anos
df_agrupado = df_pandemia.groupby('Municipio').agg({
    'NrObitosInfantis': 'sum',
    'NrNascimentos': 'sum'
}).reset_index()

# Calculando a raz√£o √≥bito/nascimento ap√≥s o agrupamento
df_agrupado['Razao_Obito_Nascimento'] = df_agrupado['NrObitosInfantis'] / df_agrupado['NrNascimentos']

# Top 10 por n√∫mero absoluto de √≥bitos infantis
top10_obitos = df_agrupado.sort_values(by='NrObitosInfantis', ascending=False).head(10)

# Top 10 por raz√£o √≥bito/nascimento
top10_razao = df_agrupado.sort_values(by='Razao_Obito_Nascimento', ascending=False).head(10)

# Plotando os gr√°ficos
fig, axes = plt.subplots(1, 2, figsize=(18, 7))

# Gr√°fico 1 ‚Äì Top 10 √≥bitos infantis
sns.barplot(
    ax=axes[0],
    data=top10_obitos,
    y='Municipio',
    x='NrObitosInfantis',
    hue='Municipio',
    palette='Reds_r',
    legend=False
)
axes[0].set_title('Top 10 Cidades ‚Äì √ìbitos Infantis (2020‚Äì2021)')
axes[0].set_xlabel('N√∫mero de √ìbitos Infantis')
axes[0].set_ylabel('Cidade')

# Gr√°fico 2 ‚Äì Top 10 raz√£o √≥bito/nascimento
sns.barplot(
    ax=axes[1],
    data=top10_razao,
    y='Municipio',
    x='Razao_Obito_Nascimento',
    hue='Municipio',
    palette='Oranges_r',
    legend=False
)
axes[1].set_title('Top 10 Cidades ‚Äì Raz√£o √ìbito/Nascimento (2020‚Äì2021)')
axes[1].set_xlabel('Raz√£o √ìbito/Nascimento')
axes[1].set_ylabel('Cidade')

plt.tight_layout()
plt.show()

# Resultados finais
cidade_maior_obito_infantil = top10_obitos.iloc[0]
cidade_maior_razao = top10_razao.iloc[0]

print("üî∏ Cidade com maior n√∫mero absoluto de √≥bitos infantis durante a pandemia (2020-2021):")
print(f"‚û°Ô∏è {cidade_maior_obito_infantil['Municipio']} - √ìbitos infantis: {cidade_maior_obito_infantil['NrObitosInfantis']}")

print("\nüî∏ Cidade com maior raz√£o de √≥bito/nascimento infantil durante a pandemia (2020-2021):")
print(f"‚û°Ô∏è {cidade_maior_razao['Municipio']} - Raz√£o: {cidade_maior_razao['Razao_Obito_Nascimento']:.4f}")

"""---

###Pergunta 02

Qual foi a varia√ß√£o no n√∫mero total de empresas entre o per√≠odo pr√©-pandemia (2018-2019) e o per√≠odo pand√™mico (2020-2021)? Que conclus√µes podemos extrair dessa an√°lise?
"""

# Definindo as colunas
coluna_empresas = 'QtEmpresas'
coluna_ano = 'Ano'

df_br[coluna_empresas] = pd.to_numeric(df_br[coluna_empresas], errors='coerce')
df_br[coluna_ano] = pd.to_numeric(df_br[coluna_ano], errors='coerce')

# Calculando totais por per√≠odos
total_pre_pandemia = df_br[df_br[coluna_ano].between(2018, 2019)][coluna_empresas].sum()
total_pandemia = df_br[df_br[coluna_ano].between(2020, 2021)][coluna_empresas].sum()

diferenca = total_pandemia - total_pre_pandemia

#Conclus√£o autom√°tica
if diferenca > 0:
    conclusao = "üöÄ Houve um aumento no n√∫mero total de empresas no per√≠odo pand√™mico (2020‚Äì2021) em compara√ß√£o ao pr√©-pandemia (2016‚Äì2019)."
elif diferenca < 0:
    conclusao = "üìâ Houve uma redu√ß√£o no n√∫mero total de empresas no per√≠odo pand√™mico (2020‚Äì2021) em compara√ß√£o ao pr√©-pandemia (2016‚Äì2019)."
else:
    conclusao = "‚ûñ N√£o houve varia√ß√£o no n√∫mero total de empresas entre os per√≠odos."

# GR√ÅFICOS EM GRID

# Dados para o gr√°fico dos per√≠odos
df_periodos = pd.DataFrame({
    'Per√≠odo': ['2018‚Äì2019 (Pr√©-pandemia)', '2020‚Äì2021 (Pandemia)'],
    'TotalEmpresas': [total_pre_pandemia, total_pandemia]
})

# Dados para o gr√°fico ano a ano
df_empresas_ano = df_br[df_br['Ano'].between(2018, 2021)].groupby('Ano')['QtEmpresas'].sum().reset_index()

# Paleta de cores
cores_periodo = ['#1f77b4', '#d62728']
cores_anos = sns.color_palette("Set2", n_colors=len(df_empresas_ano))

# Criando a grade de gr√°ficos
fig, axs = plt.subplots(1, 2, figsize=(14, 6))

# Gr√°fico 1 ‚Äì Compara√ß√£o de Per√≠odos
sns.barplot(
    data=df_periodos,
    x='Per√≠odo',
    y='TotalEmpresas',
    hue='Per√≠odo',
    palette=cores_periodo,
    dodge=False,
    legend=False,
    ax=axs[0]
)
axs[0].set_title("Total de Empresas\nPr√©-pandemia vs Pandemia", fontsize=13)
axs[0].set_xlabel("Per√≠odo", fontsize=11)
axs[0].set_ylabel("Total de Empresas", fontsize=11)

# Labels nas barras
for container in axs[0].containers:
    axs[0].bar_label(container, fmt='{:,.0f}', fontsize=9, padding=3)

# Gr√°fico 2 ‚Äì Empresas por Ano
sns.barplot(
    data=df_empresas_ano,
    x='Ano',
    y='QtEmpresas',
    hue='Ano',
    palette=cores_anos,
    dodge=False,
    legend=False,
    ax=axs[1]
)
axs[1].set_title('Total de Empresas por Ano (2018‚Äì2021)', fontsize=13)
axs[1].set_xlabel("Ano", fontsize=11)
axs[1].set_ylabel("Total de Empresas", fontsize=11)

# Labels nas barras
for container in axs[1].containers:
    axs[1].bar_label(container, fmt='{:,.0f}', fontsize=9, padding=3)

plt.tight_layout()
plt.show()

print("======================================")
print(f"Total de empresas (2018‚Äì2019): {int(total_pre_pandemia):,}")
print(f"Total de empresas (2020‚Äì2021): {int(total_pandemia):,}")
print(f"Varia√ß√£o absoluta: {int(abs(diferenca)):,}")
print("======================================")
print(conclusao)

"""Como √© poss√≠vel observar, mesmo estando em um per√≠odo complicado para a sociedade e para o mundo como um todo, o n√∫mero de empresas nos anos de 2020 e 2021 conseguiu superar o n√∫mero de empresas nos anos anteriores √† Pandemia.

---

###Pergunta 03

Qual a soma dos maiores valores de produ√ß√£o agr√≠cola por cidade no per√≠odo pr√©-pand√™mico (2016-2019) comparado ao per√≠odo pand√™mico (2020-2021)?
"""

df_br["VlProducaoAgricola"] = pd.to_numeric(df_br["VlProducaoAgricola"], errors='coerce')

df_pre_pandemia = df_br[df_br["Ano"].between(2016, 2019)]
df_pandemia = df_br[df_br["Ano"].between(2020, 2021)]

max_por_cidade_pre = df_pre_pandemia.groupby("Municipio")["VlProducaoAgricola"].max()
max_por_cidade_pand = df_pandemia.groupby("Municipio")["VlProducaoAgricola"].max()

soma_pre = max_por_cidade_pre.sum()
soma_pand = max_por_cidade_pand.sum()

# Gr√°fico
periodos = ['2016‚Äì2019', '2020‚Äì2021']
valores = [soma_pre, soma_pand]

plt.figure(figsize=(8, 8))
plt.bar(periodos, valores, color=['goldenrod', 'seagreen'])
plt.title("Soma dos maiores valores de produ√ß√£o agr√≠cola por cidade\nPr√©-pandemia vs Pandemia")
plt.ylabel("Valor da Produ√ß√£o Agr√≠cola (R$)")
for i, v in enumerate(valores):
    plt.text(i, v + (v * 0.01), f'R$ {v:,.2f}', ha='center', fontsize=11)
plt.tight_layout()
plt.show()

print(f"Soma dos maiores valores de produ√ß√£o por cidade (2016‚Äì2019): R$ {soma_pre:,.2f}")
print(f"Soma dos maiores valores de produ√ß√£o por cidade (2020‚Äì2021): R$ {soma_pand:,.2f}")

"""Como √© poss√≠vel observar, mesmo se tratandp de um per√≠odo menor (2 anos) em rela√ß√£o aos 4 anos anteriores √† pandemia (2016 a 2019), os valores de produ√ß√£o agr√≠cola conseguiram ser superiores durante a Pandemia de COVID-19.

---

###Pergunta 04

Como o crescimento do PIB dos munic√≠pios brasileiros entre 2018 e 2019 se comportou em rela√ß√£o a 2019 e 2020, in√≠cio do per√≠odo pand√™mico?
"""

df_br["PIB"] = pd.to_numeric(df_br["PIB"], errors="coerce")

pib_2018 = df_br[df_br["Ano"] == 2018]["PIB"].sum(skipna=True)
pib_2019 = df_br[df_br["Ano"] == 2019]["PIB"].sum(skipna=True)
pib_2020 = df_br[df_br["Ano"] == 2020]["PIB"].sum(skipna=True)

crescimento_pre = pib_2019 - pib_2018
crescimento_pand = pib_2020 - pib_2019

perc_pre = (crescimento_pre / pib_2018) * 100 if pib_2018 != 0 else 0
perc_pand = (crescimento_pand / pib_2020) * 100 if pib_2020 != 0 else 0

plt.figure(figsize=(8, 4))

periodos = ["2018‚Äì2019"]
valores = [perc_pre]
cores = ["dodgerblue"]

if perc_pand is not None:
    periodos.append("2019‚Äì2020")
    valores.append(perc_pand)
    cores.append("orange")

barras = plt.bar(periodos, valores, color=cores)
plt.title("Crescimento percentual do PIB dos munic√≠pios\nPr√©-pandemia vs Pandemia", fontsize=14)
plt.ylabel("Crescimento (%)", fontsize=12)

for barra in barras:
    altura = barra.get_height()
    texto = f"{altura:.2f}%"
    plt.text(barra.get_x() + barra.get_width()/2, altura + 0.5, texto, ha='center', fontsize=11)

plt.ylim(min(valores + [0]) - 2, max(valores + [0]) + 5)
plt.tight_layout()
plt.show()

print(f"PIB 2018: R$ {pib_2018:,.2f}")
print(f"PIB 2019: R$ {pib_2019:,.2f}")
print(f"Crescimento 2018‚Äì2019: R$ {crescimento_pre:,.2f} ({perc_pre:.2f}%)")

print(f"PIB 2020: R$ {pib_2020:,.2f}")
print(f"Crescimento 2019‚Äì2020: R$ {crescimento_pand:,.2f} ({perc_pand:.2f}%)")

"""---

###Pergunta 05

Existe correla√ß√£o entre receitas municipais e indicadores de qualidade de vida, como mortalidade infantil?
"""

df_br['Taxa_Mortalidade_Infantil'] = (df_br['NrObitosInfantis'] / df_br['NrNascimentos']) * 1000

correlacao = df_br[['Receitas_R$', 'Taxa_Mortalidade_Infantil']].corr().iloc[0, 1]

print("Analisando a correla√ß√£o entre a Receita Municipal e a Taxa de Mortalidade Infantil.")
print(f"A correla√ß√£o entre as vari√°veis √© de {correlacao:.2f}. Isso sugere que a rela√ß√£o entre essas vari√°veis √© {'positiva' if correlacao > 0 else 'negativa' if correlacao < 0 else 'nula'}.")
print("Se a correla√ß√£o for positiva, significa que munic√≠pios com maior receita tendem a ter maior mortalidade infantil, \nenquanto uma correla√ß√£o negativa indica o oposto.")
print("O gr√°fico a seguir ilustra essa rela√ß√£o, com a linha de tend√™ncia mostrando a correla√ß√£o linear entre as duas vari√°veis.")

plt.figure(figsize=(8, 5))
sns.scatterplot(data=df_br, x='Receitas_R$', y='Taxa_Mortalidade_Infantil')
sns.regplot(data=df_br, x='Receitas_R$', y='Taxa_Mortalidade_Infantil', scatter=False, color='red', label='Tend√™ncia Linear')

plt.title('Correla√ß√£o entre Receita Municipal e Mortalidade Infantil', fontsize=12)
plt.xlabel('Receita Municipal (R$)', fontsize=10)
plt.ylabel('Taxa de Mortalidade Infantil (por mil nascidos vivos)', fontsize=10)
plt.legend()

plt.tight_layout()
plt.show()

"""---

### Pergunta 06

H√° rela√ß√£o direta entre √°rea plantada/colhida e valor total da produ√ß√£o agr√≠cola?
"""

# 1. C√°lculo de correla√ß√µes
corr_plantada = df_br['AreaPlantada_h'].corr(df_br['VlProducaoAgricola'])
corr_colhida = df_br['AreaColhida_h'].corr(df_br['VlProducaoAgricola'])

# 2. Gr√°fico combinado
plt.figure(figsize=(10, 5))

plt.subplot(1, 2, 1)
sns.regplot(x='AreaPlantada_h', y='VlProducaoAgricola', data=df_br, scatter_kws={'alpha':0.3})
plt.title(f'√Årea Plantada (Corr: {corr_plantada:.2f})')
plt.xlabel('Hectares plantados')
plt.ylabel('Valor produ√ß√£o (R$)')

plt.subplot(1, 2, 2)
sns.regplot(x='AreaColhida_h', y='VlProducaoAgricola', data=df_br, scatter_kws={'alpha':0.3})
plt.title(f'√Årea Colhida (Corr: {corr_colhida:.2f})')
plt.xlabel('Hectares colhidos')

plt.tight_layout()
plt.show()

# 3. Sa√≠da num√©rica
print(f"\nCorrela√ß√£o √Årea Plantada x Valor Produ√ß√£o: {corr_plantada:.2f}")
print(f"Correla√ß√£o √Årea Colhida x Valor Produ√ß√£o: {corr_colhida:.2f}")

if corr_colhida > 0.7:
    print("\nConclus√£o: H√° forte rela√ß√£o direta entre √°rea e valor da produ√ß√£o")
elif corr_colhida > 0.4:
    print("\nConclus√£o: Rela√ß√£o moderada - outros fatores tamb√©m influenciam")
else:
    print("\nConclus√£o: Rela√ß√£o fraca - a √°rea n√£o √© o principal determinante do valor")

"""---

###Pergunta 07

Munic√≠pios com maior volume de exporta√ß√µes apresentam melhores indicadores econ√¥micos?
"""

# Filtro de anos
df = df_br[df_br['Ano'].between(2016, 2021)]

# Agrupamento - Exporta√ß√µes e Receitas
top_exportacoes = (
    df.groupby(['Ano', 'Municipio'])[['Exportacoes_US$', 'Receitas_R$']]
    .sum().reset_index()
)

top_receitas = (
    df.groupby(['Ano', 'Municipio'])[['Receitas_R$', 'Exportacoes_US$']]
    .sum().reset_index()
)

# Top 5 Munic√≠pios por Exporta√ß√µes (por ano)
top_exportacoes_5 = (
    top_exportacoes.sort_values(['Ano', 'Exportacoes_US$'], ascending=[True, False])
    .groupby('Ano')
    .head(5)
)

# Frequ√™ncia dos munic√≠pios no Top 5
city_export_freq = top_exportacoes_5['Municipio'].value_counts()

# Top 5 Munic√≠pios por Receitas (por ano)
top_receitas_5 = (
    top_receitas.sort_values(['Ano', 'Receitas_R$'], ascending=[True, False])
    .groupby('Ano')
    .head(5)
)

# Frequ√™ncia dos munic√≠pios no Top 5
city_receitas_freq = top_receitas_5['Municipio'].value_counts()

# Plotando Gr√°ficos em Grid
fig, axes = plt.subplots(1, 2, figsize=(16, 6))

sns.barplot(
    x=city_export_freq.index,
    y=city_export_freq.values,
    hue=city_export_freq.index,
    legend=False,
    palette='Blues_d',
    ax=axes[0]
)
axes[0].set_title('Frequ√™ncia das 5 Cidades mais Exportadoras (2016-2021)', fontsize=14)
axes[0].set_xlabel('Munic√≠pio')
axes[0].set_ylabel('N√∫mero de Apari√ß√µes')
axes[0].tick_params(axis='x', rotation=45)

sns.barplot(
    x=city_receitas_freq.index,
    y=city_receitas_freq.values,
    hue=city_receitas_freq.index,
    legend=False,
    palette='Greens_d',
    ax=axes[1]
)
axes[1].set_title('Frequ√™ncia das 5 Cidades com Maior Receita (2016-2021)', fontsize=14)
axes[1].set_xlabel('Munic√≠pio')
axes[1].set_ylabel('N√∫mero de Apari√ß√µes')
axes[1].tick_params(axis='x', rotation=45)

plt.tight_layout()
plt.show()

# Exibindo Dados em Tabelas

print("\nTop 5 Cidades por Exporta√ß√µes e suas Receitas (2016-2021):")
display(
    top_exportacoes_5[['Ano', 'Municipio', 'Exportacoes_US$', 'Receitas_R$']]
    .sort_values(by=['Ano', 'Exportacoes_US$'], ascending=[True, False])
    .style.format({'Exportacoes_US$': '${:,.2f}', 'Receitas_R$': 'R$ {:,.2f}'})
    .set_caption('Top 5 Munic√≠pios por Exporta√ß√µes')
)

print("\n\nTop 5 Cidades por Receitas e suas Exporta√ß√µes (2016-2021):")
display(
    top_receitas_5[['Ano', 'Municipio', 'Receitas_R$', 'Exportacoes_US$']]
    .sort_values(by=['Ano', 'Receitas_R$'], ascending=[True, False])
    .style.format({'Receitas_R$': 'R$ {:,.2f}', 'Exportacoes_US$': '${:,.2f}'})
    .set_caption('Top 5 Munic√≠pios por Receitas')
)

"""Como foi poss√≠vel observar nos gr√°ficos, com excess√£o de S√£o Paulo e Rio de Janeiro, nenhuma das outras cidades com maior exporta√ß√£o aparecem na lista das melhores receitas

---

###Pergunta 08

Qual foi o impacto da pandemia nos fluxos de importa√ß√£o e exporta√ß√£o por regi√£o?
"""

colunas_convert = ['Importacoes_US$', 'Exportacoes_US$', 'Ano']
for col in colunas_convert:
    df_br[col] = df_br[col].replace({'-': None, ',': '', r'\.': ''}, regex=True)
    df_br[col] = pd.to_numeric(df_br[col], errors='coerce')

# Filtrando Anos da Pandemia

df_pandemia = df_br[df_br['Ano'].isin([2019, 2020, 2021])]

# Agrupando Fluxos por Regi√£o e Ano

fluxos = (
    df_pandemia.groupby(['Ano', 'Regiao'])[['Importacoes_US$', 'Exportacoes_US$']]
    .sum().reset_index()
)

fluxos_melt = fluxos.melt(
    id_vars=['Ano', 'Regiao'],
    value_vars=['Importacoes_US$', 'Exportacoes_US$'],
    var_name='Tipo',
    value_name='Valor'
)

# Gr√°fico

plt.figure(figsize=(10, 5))

sns.barplot(
    data=fluxos_melt,
    x='Regiao',
    y='Valor',
    hue='Ano',
    palette='viridis',
    errorbar=None,
    dodge=True
)

plt.title('Impacto da Pandemia nos Fluxos de Importa√ß√£o e Exporta√ß√£o por Regi√£o')
plt.ylabel('Valor em US$')
plt.xlabel('Regi√£o')
plt.xticks(rotation=45)
plt.legend(title='Ano')
plt.tight_layout()
plt.show()

"""Como √© poss√≠vel observar, na maioria das regi√µes, com excess√£o do Centro-oeste e do Norte, os fluxos de importa√ß√£o e exporta√ß√£o retrocederam no ano de 2020, ano de in√≠cio da Pandemia.

Contudo, em todos as regi√µes, no ano seguinte, 2021, houve um aumento significativo desses indicadores.

---

---

---
---

## Associa√ß√£o de dados (Terceira Entrega)

### Munic√≠pios com maior PIB per capita e taxas de mortalidade infantil.

Este c√≥digo realiza uma an√°lise da rela√ß√£o entre o PIB per capita e a taxa de mortalidade infantil nos munic√≠pios brasileiros entre 2016 e 2021, com o objetivo de investigar como o n√≠vel de desenvolvimento econ√¥mico de uma localidade se associa a indicadores de sa√∫de p√∫blica. A escolha do PIB per capita como vari√°vel econ√¥mica se justifica por seu car√°ter padronizado, permitindo compara√ß√µes entre munic√≠pios de diferentes tamanhos populacionais. J√° a taxa de mortalidade infantil √© um indicador de qualidade de vida e acesso a servi√ßos b√°sicos, como sa√∫de, saneamento e educa√ß√£o.

A visualiza√ß√£o da rela√ß√£o entre essas vari√°veis emprega uma escala logar√≠tmica no eixo do PIB per capita, pr√°tica comum para atenuar a influ√™ncia de valores extremos e evidenciar padr√µes entre unidades com grande disparidade de renda. A transforma√ß√£o logar√≠tmica permite linearizar a rela√ß√£o, frequentemente n√£o linear, entre renda e indicadores sociais, tornando mais n√≠tida a tend√™ncia geral no conjunto dos dados. Essa abordagem √© √∫til no contexto brasileiro, onde coexistem munic√≠pios extremamente pobres e outros com alt√≠ssimos n√≠veis de renda per capita.
"""

from scipy import stats
try:
    # C√°lculo direto das vari√°veis derivadas (assumindo dados j√° limpos)
    df_br['PIB_per_capita'] = df_br['PIB'] / df_br['Populacao']
    df_br['Taxa_Mortalidade_Infantil'] = (df_br['NrObitosInfantis'] / df_br['NrNascimentos']) * 1000

    # Removendo infinitos e valores ausentes
    analysis_df = df_br[['PIB_per_capita', 'Taxa_Mortalidade_Infantil', 'Regiao', 'Ano']].replace([np.inf, -np.inf], np.nan).dropna()

    # An√°lise estat√≠stica
    print("\nEstat√≠sticas descritivas:")
    print(analysis_df[['PIB_per_capita', 'Taxa_Mortalidade_Infantil']].describe())

    correlation, p_value = stats.pearsonr(analysis_df['PIB_per_capita'], analysis_df['Taxa_Mortalidade_Infantil'])
    print(f"\nCorrela√ß√£o entre PIB per capita e Mortalidade Infantil: {correlation:.3f}")
    print(f"Valor-p: {p_value:.3f}")

    g = sns.lmplot(
    data=analysis_df,
    x='PIB_per_capita',
    y='Taxa_Mortalidade_Infantil',
    col='Regiao',
    col_wrap=3,
    palette='Set1',
    scatter_kws={'alpha': 0.5, 's': 30},
    line_kws={'color': 'black', 'linewidth': 2},
    height=5.2,
    aspect=1.1,
    robust=True
    )

    g.set(xscale="log")
    g.set_titles("{col_name}")
    g.set_axis_labels('PIB per capita (R$) - Escala log', 'Taxa de Mortalidade Infantil (por 1000 nascidos)')

    g.fig.subplots_adjust(top=0.88, hspace=0.4)

    g.fig.suptitle(
        'Rela√ß√£o entre PIB per capita e Mortalidade Infantil por Regi√£o (2016-2021)',
        fontsize=16
    )

    plt.show()

except Exception as e:
    print(f"\nErro durante a an√°lise: {str(e)}")
    print("Verifique se todas as colunas necess√°rias existem no DataFrame:")
    print(df_br.columns.tolist())

"""A aplica√ß√£o do teste de correla√ß√£o de Pearson fornece uma medida estat√≠stica da associa√ß√£o linear entre PIB per capita e taxa de mortalidade infantil. Um coeficiente negativo obtido no teste indica que h√° uma tend√™ncia inversa entre as vari√°veis: munic√≠pios com maior renda per capita tendem a registrar menores taxas de mortalidade infantil. O valor de p associado ao teste confirma a signific√¢ncia estat√≠stica dessa correla√ß√£o, sugerindo que o padr√£o observado dificilmente ocorreu por acaso.

A distin√ß√£o regional no gr√°fico, representada por cores diferentes para cada macrorregi√£o brasileira, desempenha um papel essencial na interpreta√ß√£o dos dados. Ela permite observar como regi√µes historicamente mais vulner√°veis, como o Norte e o Nordeste, concentram maior dispers√£o nas taxas de mortalidade infantil, especialmente entre os munic√≠pios com menor PIB per capita. J√° as regi√µes Sul e Sudeste, com maior densidade de munic√≠pios mais ricos, tendem a apresentar taxas mais homog√™neas e reduzidas, refor√ßando desigualdades estruturais no pa√≠s.

O padr√£o visual evidenciado no gr√°fico confirma a hip√≥tese de que melhores condi√ß√µes econ√¥micas est√£o associadas a melhores desfechos sociais. No entanto, a presen√ßa de outliers,munic√≠pios com alta renda e mortalidade elevada, ou com baixa renda e mortalidade surpreendentemente baixa,revela que o PIB per capita, embora relevante, n√£o √© o √∫nico determinante da sa√∫de infantil. Fatores como pol√≠ticas p√∫blicas locais, distribui√ß√£o interna da renda, qualidade dos servi√ßos de sa√∫de e vari√°veis ambientais podem distorcer a rela√ß√£o geral, indicando caminhos para an√°lises futuras mais detalhadas e multivariadas.

---

### Densidade Econ√¥mica vs. Produtividade

Esse c√≥digo realiza uma an√°lise da rela√ß√£o entre popula√ß√£o e PIB utilizando uma transforma√ß√£o logar√≠tmica, fundamentada na Lei de Zipf, padr√£o emp√≠rico que descreve a distribui√ß√£o de grandezas econ√¥micas e demogr√°ficas em sistemas urbanos e regionais. A ado√ß√£o da escala log-log nos eixos do gr√°fico serve para linearizar a rela√ß√£o entre essas vari√°veis, j√° que o PIB tipicamente apresenta crescimento n√£o linear em fun√ß√£o da popula√ß√£o. Ao converter ambos os valores para logaritmo na base 10, a an√°lise transforma uma curva potencialmente exponencial em uma rela√ß√£o pass√≠vel de interpreta√ß√£o linear, facilitando a identifica√ß√£o de padr√µes e compara√ß√µes regionais.
"""

# C√°lculo de PIB per capita
df_br['PIB_per_capita'] = df_br['PIB'] / df_br['Populacao']

# Filtrando dados v√°lidos para log (evita log de zero ou negativo)
df_valid = df_br[(df_br['Populacao'] > 0) & (df_br['PIB'] > 0)]

log_pop = np.log10(df_valid['Populacao'])
log_pib = np.log10(df_valid['PIB'])

# Gr√°fico hexbin
plt.figure(figsize=(10, 8))
plt.hexbin(
    x=log_pop,
    y=log_pib,
    gridsize=50,
    cmap='Reds',
    mincnt=1
)
plt.colorbar(label='Contagem de cidades')
plt.title('Densidade Popula√ß√£o vs PIB (Escala Log-Log)')
plt.xlabel('log10(Popula√ß√£o)')
plt.ylabel('log10(PIB)')
plt.grid(True)
plt.show()

# Correla√ß√£o Popula√ß√£o vs PIB
corr_pop_pib, p_pop_pib = stats.pearsonr(log_pop, log_pib)
print(f"Correla√ß√£o Popula√ß√£o-PIB (log): {corr_pop_pib:.3f} (p={p_pop_pib:.3f})")


# Correla√ß√£o parcial PIB per capita vs Mortalidade, controlando por Regi√£o
df_corr = df_br.dropna(subset=['PIB_per_capita', 'Taxa_Mortalidade_Infantil', 'Regiao'])

taxa_mortalidade_residual = (
    df_corr['Taxa_Mortalidade_Infantil'] - df_corr.groupby('Regiao')['Taxa_Mortalidade_Infantil'].transform('mean')
)

partial_corr = stats.pearsonr(df_corr['PIB_per_capita'], taxa_mortalidade_residual)
print(f"\nCorrela√ß√£o PIB-Mortalidade controlada por regi√£o: {partial_corr[0]:.3f} (p={partial_corr[1]:.3f})")

"""O teste de correla√ß√£o de Pearson, aplicado √†s vari√°veis log-transformadas, quantifica a intensidade e a dire√ß√£o da associa√ß√£o linear entre popula√ß√£o e PIB. Um coeficiente pr√≥ximo de 1 revela uma correla√ß√£o positiva robusta, indicando que, em m√©dia, o aumento populacional est√° associado a um crescimento proporcional do PIB. O valor de p correspondente atesta a signific√¢ncia estat√≠stica dessa rela√ß√£o, descartando a possibilidade de que o padr√£o observado seja aleat√≥rio.

A diferencia√ß√£o por cores das regi√µes no gr√°fico cumpre um papel cr√≠tico na visualiza√ß√£o, permitindo detectar desvios da tend√™ncia geral. Por exemplo, √°reas com PIB significativamente maior ou menor do que o previsto para seu porte populacional tornam-se imediatamente identific√°veis, sugerindo disparidades na produtividade ou na estrutura econ√¥mica regional.

Os resultados gr√°ficos exibem uma clara tend√™ncia linear ascendente na dispers√£o dos pontos, corroborando a hip√≥tese de que regi√µes mais populosas tendem a gerar PIBs mais elevados. Contudo, os outliers presentes ‚Äì como √°reas com alta produtividade econ√¥mica apesar de popula√ß√µes reduzidas, ou vice-versa ‚Äì merecem aten√ß√£o especial. Esses casos podem refletir din√¢micas econ√¥micas particulares, como a presen√ßa de polos industriais concentrados, vantagens log√≠sticas ou depend√™ncia de recursos naturais, que distorcem a rela√ß√£o geral entre popula√ß√£o e produ√ß√£o econ√¥mica.

---

### Efici√™ncia Agr√≠cola por Regi√£o

Para analisar a efici√™ncia agr√≠cola nas diferentes regi√µes do Brasil, foi utilizada uma abordagem de associa√ß√£o entre vari√°veis categ√≥ricas,regi√µes, e uma vari√°vel cont√≠nua ,efici√™ncia agr√≠cola. A efici√™ncia agr√≠cola foi definida como a raz√£o entre o valor da produ√ß√£o agr√≠cola ,em reais, e a √°rea plantada ,em hectares, representando o rendimento econ√¥mico por hectare cultivado. Essa m√©trica foi escolhida por refletir diretamente o desempenho produtivo das regi√µes em termos de retorno financeiro por √°rea utilizada.

No tratamento dos dados, foi criado um novo atributo chamado Eficiencia_Agricola por meio da divis√£o do valor da produ√ß√£o pela √°rea plantada (df_br['VlProducaoAgricola'] / df_br['AreaPlantada_h']). Essa transforma√ß√£o permitiu padronizar o desempenho agr√≠cola, tornando poss√≠vel comparar regi√µes com diferentes tamanhos e volumes de produ√ß√£o de forma proporcional.

Para investigar a associa√ß√£o entre a regi√£o e a efici√™ncia agr√≠cola, foram extra√≠dos os dados de efici√™ncia por regi√£o, agrupando-os com base na vari√°vel categ√≥rica Regiao. A escolha dessa estrutura permitiu isolar o comportamento de cada grupo de forma independente, condi√ß√£o necess√°ria para a aplica√ß√£o de um teste estat√≠stico de compara√ß√£o entre grupos. Foi ent√£o utilizado o teste ANOVA ,An√°lise de Vari√¢ncia, que √© indicado para avaliar se existem diferen√ßas estatisticamente significativas nas m√©dias da vari√°vel cont√≠nua entre tr√™s ou mais grupos categ√≥ricos. No c√≥digo, as listas com os dados de efici√™ncia agr√≠cola por regi√£o foram passadas como par√¢metros para a fun√ß√£o f_oneway, da biblioteca scipy.stats, que retornou um valor F e um p-valor.

A estat√≠stica F indica o grau de separa√ß√£o entre as m√©dias dos grupos em rela√ß√£o √† variabilidade interna de cada grupo. J√° o p-valor informa a probabilidade de se obter essas diferen√ßas observadas por acaso. Caso o p-valor seja menor que um n√≠vel de signific√¢ncia adotado, podemos afirmar que h√° diferen√ßas estat√≠sticas significativas na efici√™ncia agr√≠cola entre ao menos duas regi√µes.
"""

df_br['Eficiencia_Agricola'] = df_br['VlProducaoAgricola'] / df_br['AreaPlantada_h']

Q1 = df_br['Eficiencia_Agricola'].quantile(0.25)
Q3 = df_br['Eficiencia_Agricola'].quantile(0.75)
IQR = Q3 - Q1

# Limites para detec√ß√£o de outliers
limite_inferior = Q1 - 1.5 * IQR
limite_superior = Q3 + 1.5 * IQR

# Filtra os dados sem outliers
df_sem_outliers = df_br[
    (df_br['Eficiencia_Agricola'] >= limite_inferior) &
    (df_br['Eficiencia_Agricola'] <= limite_superior)
]

plt.figure(figsize=(8,4))
sns.boxplot(
    x='Regiao',
    y='Eficiencia_Agricola',
    hue='Regiao',
    data=df_sem_outliers,
    palette='viridis',
    legend=False
)
plt.yscale('log')

plt.title('Efici√™ncia Agr√≠cola (R$/hectare) por Regi√£o (Outliers Removidos)')
plt.ylabel('Efici√™ncia Agr√≠cola (Log R$/hectare)')
plt.xlabel('Regi√£o')
plt.xticks(rotation=45)
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

"""O boxplot gerado com os resultados refor√ßa essa an√°lise. Observa-se que, embora todas as regi√µes apresentem uma grande quantidade de valores concentrados pr√≥ximos da base, indicando baixa efici√™ncia na maioria dos casos, h√° uma dispers√£o not√°vel de outliers em todas elas, com destaque para o Sudeste e Sul, que mostram maiores amplitudes nos valores extremos. Essa visualiza√ß√£o confirma a exist√™ncia de variabilidade significativa dentro de cada grupo, mas tamb√©m sugere padr√µes distintos entre as regi√µes. A presen√ßa desses outliers e a largura dos boxes, que representam os intervalos interquartis, indicam diferen√ßas estruturais na produtividade agr√≠cola que justificam a aplica√ß√£o do teste estat√≠stico.

---

###An√°lise Espacial da Letalidade da COVID-19 no Brasil

Nesta subse√ß√£o, realizamos uma an√°lise da letalidade da COVID-19 nos estados brasileiros, considerando a propor√ß√£o de √≥bitos em rela√ß√£o ao total de casos confirmados (deaths_by_totalCases). Para tornar a compara√ß√£o mais justa entre estados com diferentes tamanhos populacionais, essa taxa foi ajustada pela popula√ß√£o estimada de cada estado.

A visualiza√ß√£o foi feita por meio de mapas tem√°ticos do Brasil para os anos de 2020 a 2023, com cores representando a intensidade da letalidade ajustada. Assim, conseguimos observar padr√µes espaciais e temporais da pandemia no pa√≠s, permitindo compreender como a situa√ß√£o evoluiu em diferentes regi√µes ao longo do tempo.

Essa an√°lise √© √∫til para destacar disparidades regionais, identificar estados com maior ou menor impacto e fornecer insumos para estudos epidemiol√≥gicos e decis√µes de pol√≠tica p√∫blica
"""

# Agrupar df_todos_max por state e ano
df_state_year = df_todos_max.groupby(['state', 'ano'])['deaths_by_totalCases'].mean().reset_index()

# Pegar a popula√ß√£o mais recente de cada estado no df_br
df_pop = df_br.sort_values('Ano', ascending=False).drop_duplicates('UF')[['UF', 'Populacao']]

# Renomear para poder juntar
df_pop.rename(columns={'UF': 'state', 'Populacao': 'pop'}, inplace=True)

# Juntar os dados
df_merged = df_state_year.merge(df_pop, on='state')

# Calcular taxa ajustada pela popula√ß√£o --> corre√ß√£o utilizando 10k habitantes -> cerca de 70% dos munic√≠pios BR tem menos que 20k de hab.
df_merged['deaths_by_cases_per_10k_pop'] = (df_merged['deaths_by_totalCases'] / df_merged['pop']) * 10000

# Carregar mapa do Brasil por estados
estados_for_merge = estados.copy()
estados_for_merge.rename(columns={'sigla': 'state'}, inplace=True)

# Loop por ano
anos = [2020, 2021, 2022, 2023]
fig, axs = plt.subplots(2, 2, figsize=(16, 12))
axs = axs.flatten()


for i, ano in enumerate(anos):
    ax = axs[i]
    df_ano = df_merged[df_merged['ano'] == ano]

    mapa_dados = estados_for_merge.merge(df_ano, on='state', how='left')

    mapa_dados['deaths_by_cases_per_10k_pop'] = mapa_dados['deaths_by_cases_per_10k_pop'].fillna(0)

    mapa_dados.plot(
        column='deaths_by_cases_per_10k_pop',
        cmap='OrRd',
        linewidth=0.8,
        ax=ax,
        edgecolor='0.8',
        legend=True,
        legend_kwds={'label': "Taxa ajustada por popula√ß√£o", 'shrink': 0.6}
    )

    ax.set_title(f"Taxa de mortes por casos (ajustada) - {ano}")
    ax.axis('off')

plt.tight_layout()
plt.show()

"""Como √© poss√≠vel observar, com o passar dos anos a taxa de mortes foi diminuindo significativamente.
<p>Alguns fatores podem ter contribu√≠do com isso e precisam ser levados em considera√ß√£o, como o aumento da campanha de vacina√ß√£o e a melhor prepara√ß√£o das cidades/regi√µes para lidar com a doen√ßa j√° em seu terceiro ano (2023).

####Estat√≠ticas e Teorema Central do Limite

Antes de aprofundarmos as infer√™ncias e correla√ß√µes, √© essencial compreender os fundamentos estat√≠sticos dos dados analisados. Esta se√ß√£o apresenta os seguintes conceitos:

* Distribui√ß√µes e Histograma

* Fun√ß√£o de Distribui√ß√£o Acumulada

* Teorema Central do Limite
"""

import seaborn as sns

sns.histplot(df_merged['deaths_by_cases_per_10k_pop'], bins=20, kde=True)
plt.title('Distribui√ß√£o da taxa de mortes por casos (ajustada)')
plt.xlabel('Taxa por 10k habitantes')
plt.ylabel('Frequ√™ncia')
plt.show()

"""Observamos uma distribui√ß√£o fortemente assim√©trica positiva, com:

* muitos munic√≠pios concentrados abaixo de 0.02 mortes/10k hab (indicador de baixa letalidade na maioria das localidades)

* Cauda longa estendendo-se at√© ~0.10 mortes/10k hab, representando munic√≠pios com taxas excepcionalmente altas

Essa assimetria, vis√≠vel tanto no histograma quanto na curva de densidade (linha azul), sugere que:

* A maioria dos munic√≠pios teve impacto proporcionalmente menor da pandemia
* Os outliers √† direita podem representar:
 * Subnotifica√ß√£o de casos
 * Colapsos locais do sistema de sa√∫de
 * Caracter√≠sticas demogr√°ficas espec√≠ficas (ex.: cidades com popula√ß√µes mais idosas)

A an√°lise separada por faixas populacionais √© recomendada, pois munic√≠pios muito pequenos podem distorcer as taxas

---

"""

import numpy as np

sorted_data = np.sort(df_merged['deaths_by_cases_per_10k_pop'])
cdf = np.arange(len(sorted_data)) / float(len(sorted_data))

plt.plot(sorted_data, cdf, marker='.', linestyle='none')
plt.title('Fun√ß√£o de Distribui√ß√£o Acumulada (CDF)')
plt.xlabel('Taxa ajustada')
plt.ylabel('Probabilidade acumulada')
plt.grid(True)
plt.show()

"""A CDF (Cumulative Distribution Function) mostra a probabilidade acumulada de se encontrar uma taxa de mortalidade menor ou igual a um determinado valor. Aqui, vemos que aproximadamente:

1.   Comportamento Inicial √çngreme (0.00 a 0.02):
 * A curva sobe rapidamente no in√≠cio, indicando que aproximadamente 60-70% dos munic√≠pios t√™m taxas ‚â§ 0.02 mortes/10k hab

2.   Ponto de Inflex√£o em ~0.025:
 * Onde a curva come√ßa a se tornar menos √≠ngreme
 * Indica o limite onde a maioria dos munic√≠pios "t√≠picos" se concentra
3.   Cauda Longa (0.04 a 0.10):
 * A curva ascende lentamente, mostrando que poucos munic√≠pios t√™m taxas mais altas
 * Aproximadamente 90% dos munic√≠pios est√£o abaixo de 0.06 mortes/10k hab


Essa fun√ß√£o √© √∫til para entender a propor√ß√£o de munic√≠pios abaixo de determinados limiares, o que pode auxiliar na defini√ß√£o de pol√≠ticas p√∫blicas.

---
"""

import random

amostras = []
for _ in range(1000):
    amostra = df_merged['deaths_by_cases_per_10k_pop'].sample(n=10)
    amostras.append(amostra.mean())

sns.histplot(amostras, kde=True)
plt.title('Distribui√ß√£o da m√©dia das amostras (TCL)')
plt.xlabel('M√©dia da taxa ajustada (10k hab)')
plt.ylabel('Frequ√™ncia')
plt.show()

"""A distribui√ß√£o das m√©dias amostrais da taxa de mortalidade segue um padr√£o aproximadamente normal, como previsto pelo Teorema Central do Limite. Isso confirma que, mesmo que a distribui√ß√£o original seja enviesada, a m√©dia das amostras se comporta de maneira previs√≠vel e sim√©trica, permitindo a aplica√ß√£o de t√©cnicas inferenciais com maior confiabilidade.

####Infer√™ncia Estat√≠stica

A correla√ß√£o √© uma t√©cnica estat√≠stica usada para medir o grau de associa√ß√£o entre duas vari√°veis num√©ricas. No nosso caso, buscamos entender se existe alguma rela√ß√£o entre a popula√ß√£o de um estado e a taxa de mortes ajustada por 10 mil habitantes.

Essa an√°lise nos ajuda a responder perguntas como:

"Estados mais populosos tiveram taxas de mortalidade maiores ou menores proporcionalmente √† sua popula√ß√£o?"

A medida utilizada √© o coeficiente de correla√ß√£o de Pearson, que varia entre:

* -1: correla√ß√£o negativa perfeita

* 0: nenhuma correla√ß√£o

* +1: correla√ß√£o positiva perfeita

Al√©m disso, usamos o valor-p (p-value) para saber se a correla√ß√£o encontrada √© estatisticamente significativa.

---
Correla√ß√£o entre Popula√ß√£o e Taxa de Mortalidade por Casos:
"""

# Pr√©-processamento dos dados
df_corr = df_merged[['pop', 'deaths_by_cases_per_10k_pop']].copy()

# Remover linhas com valores faltantes
df_corr = df_corr.dropna()

# Substituir infinitos (caso existam)
df_corr = df_corr.replace([np.inf, -np.inf], np.nan).dropna()

# Verificar se h√° dados suficientes
if len(df_corr) < 3:
    raise ValueError("Dados insuficientes para calcular correla√ß√£o (m√≠nimo 3 observa√ß√µes)")

# Calcular correla√ß√£o com tratamento de erros
try:
    corr, p = pearsonr(df_corr['pop'], df_corr['deaths_by_cases_per_10k_pop'])
    print("\nCorrela√ß√£o entre Popula√ß√£o e Taxa de Mortalidade por Casos (ajustada)")
    print("="*60)
    print(f"Coeficiente de Pearson (r): {corr:.4f}")
    print(f"Valor-p: {p:.4f}")
    print(f"N√∫mero de observa√ß√µes v√°lidas: {len(df_corr)}")

    # Interpreta√ß√£o do valor
    if abs(corr) < 0.2:
        strength = "muito fraca"
    elif 0.2 <= abs(corr) < 0.4:
        strength = "fraca"
    elif 0.4 <= abs(corr) < 0.6:
        strength = "moderada"
    elif 0.6 <= abs(corr) < 0.8:
        strength = "forte"
    else:
        strength = "muito forte"

    direction = "positiva" if corr > 0 else "negativa"

    print(f"\nInterpreta√ß√£o: Correla√ß√£o {direction} {strength}", end=" ")
    print("(estatisticamente significativa)" if p < 0.05 else "(n√£o significativa)")

except Exception as e:
    print(f"\nErro ao calcular correla√ß√£o: {str(e)}")

"""---
Compara√ß√£o da Mortalidade Proporcional por Casos entre as Regi√µes Norte e Sul:

Nesta an√°lise, comparamos dois grupos de estados (por exemplo, Norte e Sul) para verificar se a m√©dia das taxas de mortes ajustadas difere significativamente entre eles.

A t√©cnica usada √© o teste t para duas amostras independentes (t-test), que avalia se as m√©dias de dois grupos s√£o estatisticamente diferentes.

Esse teste √© muito usado em ci√™ncias para comparar grupos distintos (ex.: tratamento vs controle, antes vs depois, regi√£o A vs regi√£o B).

üîπ Como Funciona:

* Calcula a m√©dia das taxas para cada grupo.

* Verifica se a diferen√ßa entre as m√©dias pode ter ocorrido ao acaso.

* Se o valor-p for menor que 0.05, significa que a diferen√ßa √© estatisticamente significativa.
"""

from scipy.stats import ttest_ind

regioes = {
    'Norte': ['AC', 'AP', 'AM', 'PA', 'RO', 'RR', 'TO'],
    'Sul': ['RS', 'SC', 'PR']
}

df_merged['regiao'] = df_merged['state'].apply(
    lambda x: 'Norte' if x in regioes['Norte'] else
             ('Sul' if x in regioes['Sul'] else np.nan)
)

df_filtrado = df_merged[
    df_merged['regiao'].isin(['Norte', 'Sul'])
].dropna(subset=['deaths_by_cases_per_10k_pop'])

df_filtrado = df_filtrado.replace([np.inf, -np.inf], np.nan).dropna()

print("\nTamanho dos grupos:")
print(df_filtrado['regiao'].value_counts())

try:
    grupo_norte = df_filtrado[df_filtrado['regiao'] == 'Norte']['deaths_by_cases_per_10k_pop']
    grupo_sul = df_filtrado[df_filtrado['regiao'] == 'Sul']['deaths_by_cases_per_10k_pop']

    # Teste t com Welch's correction (n√£o assume vari√¢ncias iguais)
    t_stat, p_value = ttest_ind(grupo_norte, grupo_sul, equal_var=False, nan_policy='omit')

    print("\nResultado do Teste t para Amostras Independentes:")
    print("="*60)
    print(f"Estados do Norte (n={len(grupo_norte)}): M√©dia = {grupo_norte.mean():.4f}")
    print(f"Estados do Sul (n={len(grupo_sul)}): M√©dia = {grupo_sul.mean():.4f}")
    print(f"\nDiferen√ßa absoluta: {abs(grupo_norte.mean() - grupo_sul.mean()):.4f}")
    print(f"Estat√≠stica t: {t_stat:.4f}")
    print(f"Valor-p: {p_value:.4f}")

    if p_value < 0.05:
        print("\nConclus√£o: H√° diferen√ßa estatisticamente significativa (p < 0.05)")
        if grupo_norte.mean() > grupo_sul.mean():
            print("A regi√£o Norte apresenta maior mortalidade proporcional que a Sul")
        else:
            print("A regi√£o Sul apresenta maior mortalidade proporcional que a Norte")
    else:
        print("\nConclus√£o: N√£o h√° diferen√ßa estatisticamente significativa (p ‚â• 0.05)")

except Exception as e:
    print(f"\nErro ao realizar o teste t: {str(e)}")

"""####Regras de Associa√ß√£o

As regras de associa√ß√£o s√£o t√©cnicas amplamente usadas na minera√ß√£o de dados para descobrir padr√µes frequentes entre elementos que aparecem juntos em um conjunto de dados.

Um exemplo famoso de uso √© no varejo:

 -- "Se o cliente compra p√£o e leite, ent√£o ele provavelmente comprar√° manteiga."

No nosso caso, aplicamos essa l√≥gica para entender padr√µes como:

* Estados com popula√ß√£o pequena tendem a ter taxas de mortalidade altas?

* Regi√µes com baixa taxa tamb√©m t√™m baixa popula√ß√£o?

üîπ Como Funciona:

1 - Cada linha √© tratada como uma transa√ß√£o com m√∫ltiplos itens (ex: "popula√ß√£o baixa", "taxa alta").

2 - O algoritmo Apriori identifica quais combina√ß√µes ocorrem com frequ√™ncia.

3 - Gera regras do tipo:
Se: {pop_pequena}, Ent√£o: {taxa_alta}

-- E calcula 3 medidas importantes:

* Suporte: frequ√™ncia com que a regra aparece.

* Confian√ßa: chance de a consequ√™ncia acontecer dado o antecedente.

* Lift: for√ßa da regra em rela√ß√£o ao acaso (acima de 1 = rela√ß√£o positiva).
"""

from mlxtend.frequent_patterns import apriori, association_rules
from mlxtend.preprocessing import TransactionEncoder
import pandas as pd

# Remover valores ausentes e reiniciar √≠ndices
df_assoc = df_merged[['pop', 'deaths_by_cases_per_10k_pop']].dropna().reset_index(drop=True)

# Classifica√ß√£o com qcut e extra√ß√£o dos intervalos (bins)
pop_bins, pop_intervals = pd.qcut(df_assoc['pop'], q=3, labels=['pop_baixa', 'pop_media', 'pop_alta'], retbins=True, duplicates='drop')
taxa_bins, taxa_intervals = pd.qcut(df_assoc['deaths_by_cases_per_10k_pop'], q=3, labels=['taxa_baixa', 'taxa_media', 'taxa_alta'], retbins=True, duplicates='drop')

# Adicionar colunas categ√≥ricas ao dataframe
df_assoc['pop_cat'] = pop_bins
df_assoc['taxa_cat'] = taxa_bins

# Mostrar os intervalos usados nas categorias
print(" Intervalos de popula√ß√£o utilizados:")
for i, cat in enumerate(['pop_baixa', 'pop_media', 'pop_alta']):
    print(f"{cat}: ({pop_intervals[i]:,.0f} - {pop_intervals[i+1]:,.0f}) habitantes")

print("\n Intervalos de taxa de letalidade por 10 mil habitantes:")
for i, cat in enumerate(['taxa_baixa', 'taxa_media', 'taxa_alta']):
    print(f"{cat}: ({taxa_intervals[i]:.2f} - {taxa_intervals[i+1]:.2f})")

# Criar transa√ß√µes
transacoes = df_assoc[['pop_cat', 'taxa_cat']].astype(str).values.tolist()

# Codificar as transa√ß√µes
te = TransactionEncoder()
te_ary = te.fit(transacoes).transform(transacoes)
df_trans = pd.DataFrame(te_ary, columns=te.columns_)

# Aplicar Apriori
frequentes = apriori(df_trans, min_support=0.05, use_colnames=True)
regras = association_rules(frequentes, metric="confidence", min_threshold=0.5)

# Exibir as regras encontradas
print("\n Regras de Associa√ß√£o Encontradas:")
if regras.empty:
    print("Nenhuma regra encontrada com os par√¢metros definidos.")
else:
    print(regras[['antecedents', 'consequents', 'support', 'confidence', 'lift']])

"""1.   Intervalos Utilizados:
* As faixas de popula√ß√£o foram divididas em tr√™s grupos:
    * pop_baixa: at√© ~8 mil habitantes;
    * pop_media: de ~8 mil a ~17 mil;
    * pop_alta: acima de ~17 mil habitantes (indo at√© mais de 3 milh√µes).

* As faixas de taxa de letalidade por 10 mil habitantes foram:
    * taxa_baixa: at√© 0.01;
    * taxa_media: entre 0.01 e 0.03;
    * taxa_alta: acima de 0.03.

2.   Conclus√£o:
As regras encontradas indicam que h√° uma associa√ß√£o clara entre o tamanho da popula√ß√£o de uma cidade e a taxa de letalidade por Covid-19. Em geral:

 * Cidades maiores tendem a ter baixa letalidade;

 * Cidades pequenas tendem a ter maior letalidade;

 * H√° uma correspond√™ncia intermedi√°ria para o grupo m√©dio.

* Esses padr√µes podem indicar quest√µes relacionadas √† estrutura de sa√∫de, capacidade de atendimento ou subnotifica√ß√£o nas cidades menores.

---
---

---
---

# Regress√£o Linear (Quarta Entrega)

Nesta etapa do projeto, realizamos uma an√°lise de regress√£o linear com o objetivo de estimar vari√°veis num√©ricas relevantes do conjunto de dados BrStats, que re√∫ne informa√ß√µes socioecon√¥micas e demogr√°ficas de munic√≠pios brasileiros. A regress√£o foi escolhida por ser uma t√©cnica amplamente utilizada para identificar rela√ß√µes entre vari√°veis e realizar previs√µes com base em padr√µes observados.

Para garantir a qualidade dos modelos, adotamos um processo cuidadoso que incluiu:

* **Escolha fundamentada das vari√°veis-alvo e preditoras**, com base em an√°lises anteriores e conhecimento sobre o contexto dos dados;

* **Pr√©-processamento avan√ßado, envolvendo limpeza de dados**, transforma√ß√£o de vari√°veis (como logaritmiza√ß√£o) e remo√ß√£o de inconsist√™ncias;

* **Verifica√ß√£o de multicolinearidade entre as vari√°veis independentes**, utilizando o VIF (Fator de Infla√ß√£o da Vari√¢ncia) para evitar redund√¢ncias que comprometam o modelo;

* **Ajuste dos modelos com erros padr√£o robustos**, aumentando a confiabilidade estat√≠stica dos resultados;

* **Avalia√ß√£o gr√°fica e estat√≠stica dos modelos**, com an√°lises de res√≠duos, QQ-plots e compara√ß√µes entre valores previstos e reais;

* **Interpreta√ß√£o dos coeficientes obtidos**, visando extrair conclus√µes sobre o impacto de cada vari√°vel preditora no comportamento da vari√°vel-alvo.

Ao longo desta se√ß√£o, ser√£o apresentados diferentes modelos de regress√£o, com objetivos variados, sempre acompanhados de an√°lises cr√≠ticas dos resultados e de sua signific√¢ncia pr√°tica e estat√≠stica.

---

## PIB em fun√ß√£o de vari√°veis econ√¥micas

Neste trecho, o objetivo principal foi ajustar um modelo de regress√£o linear para prever o Produto Interno Bruto (PIB) dos munic√≠pios com base em vari√°veis socioecon√¥micas e produtivas. Inicialmente, o conjunto de dados passou por um processo de prepara√ß√£o avan√ßada, no qual foram selecionadas vari√°veis relacionadas √† produ√ß√£o agr√≠cola e pecu√°ria, com√©rcio exterior (importa√ß√µes e exporta√ß√µes), receitas p√∫blicas e n√∫mero de pessoas ocupadas. Todas essas vari√°veis, incluindo o PIB, foram transformadas com a fun√ß√£o logar√≠tmica log1p, uma t√©cnica comumente usada para lidar com distribui√ß√µes assim√©tricas e diferen√ßas de escala entre os dados.

Ap√≥s o pr√©-processamento, as vari√°veis explicativas passaram por uma an√°lise de multicolinearidade utilizando o Fator de Infla√ß√£o da Vari√¢ncia (VIF). Com base nos resultados, foram removidas algumas vari√°veis que apresentavam colinearidade elevada, garantindo a estabilidade e interpretabilidade do modelo final.

Com os dados prontos, foi ajustado um modelo de regress√£o linear robusto (com erros padr√£o do tipo HC3, resistentes √† heterocedasticidade). A seguir, foram gerados tr√™s gr√°ficos diagn√≥sticos fundamentais: o gr√°fico de res√≠duos versus valores ajustados (para verificar a linearidade e homocedasticidade), o QQ-plot (para avaliar a normalidade dos res√≠duos) e o histograma da distribui√ß√£o dos res√≠duos (que oferece uma vis√£o geral do comportamento dos erros do modelo).

Por fim, foi apresentada uma visualiza√ß√£o comparando os valores reais e previstos do PIB na escala original, incluindo uma linha de refer√™ncia de previs√£o perfeita. Essa visualiza√ß√£o permite avaliar de forma intuitiva a capacidade preditiva do modelo em termos absolutos, fornecendo insights pr√°ticos sobre sua performance.
"""

def check_vif(X):
    """Calcula VIF para identificar multicolinearidade"""
    vif_data = pd.DataFrame()
    vif_data["Vari√°vel"] = X.columns
    vif_data["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]
    return vif_data.sort_values("VIF", ascending=False)


def prepare_data(df):
    """Prepara dados com log e remove outliers"""
    df = df.copy()

    features = ['PessoalOcupado', 'VrSalarios', 'QtEmpresas',
                'VlProducaoAgricola', 'VlProducaoPecuaria',
                'Importacoes_US$', 'Exportacoes_US$',
                'Receitas_R$', 'Transferencias_correntes_R$',
                'Transferencias_capital_R$']

    for col in features + ['PIB']:
        df[f'log_{col}'] = np.log1p(df[col])

    df = df.replace([np.inf, -np.inf], np.nan).dropna()

    return df


# Prepara√ß√£o dos Dados

df_prepared = prepare_data(df_br)

# Sele√ß√£o de vari√°veis
X = df_prepared[[f'log_{col}' for col in [
    'PessoalOcupado', 'VrSalarios', 'QtEmpresas',
    'Importacoes_US$', 'Exportacoes_US$', 'Receitas_R$']]]

y = df_prepared['log_PIB']

# Verificar Multicolinearidade

vif_table = check_vif(X)
print("\n=== An√°lise de Multicolinearidade (VIF) ===")
print(vif_table)

# Ajuste do Modelo

X_const = sm.add_constant(X)
modelo_pib = sm.OLS(y, X_const).fit(cov_type='HC3')

# Resultados em Tabela

summary_table = modelo_pib.summary2().tables[1]
print("\n=== Resumo do Modelo ===")
print(summary_table)

# Plot de Diagn√≥stico (em grid)

fig, axs = plt.subplots(2, 2, figsize=(14, 10))

# Res√≠duos vs Ajustados
sns.scatterplot(x=modelo_pib.fittedvalues, y=modelo_pib.resid, ax=axs[0, 0])
axs[0, 0].axhline(y=0, color='red', linestyle='--')
axs[0, 0].set_title('Res√≠duos vs Ajustados')
axs[0, 0].set_xlabel('Valores Ajustados')
axs[0, 0].set_ylabel('Res√≠duos')

# QQ Plot
stats.probplot(modelo_pib.resid, dist="norm", plot=axs[0, 1])
axs[0, 1].set_title('QQ-Plot dos Res√≠duos')

# Distribui√ß√£o dos Res√≠duos
sns.histplot(modelo_pib.resid, kde=True, ax=axs[1, 0])
axs[1, 0].set_title('Distribui√ß√£o dos Res√≠duos')
axs[1, 0].set_xlabel('Res√≠duos')

# PIB Real vs PIB Previsto
sns.regplot(
    x=np.expm1(modelo_pib.predict(X_const)),
    y=np.expm1(y),
    line_kws={'color': 'red'},
    scatter_kws={'alpha': 0.3},
    ax=axs[1, 1]
)
axs[1, 1].plot(
    [np.expm1(y).min(), np.expm1(y).max()],
    [np.expm1(y).min(), np.expm1(y).max()],
    'k--', label='Perfeita previs√£o'
)
axs[1, 1].set_title('PIB Real vs PIB Previsto')
axs[1, 1].set_xlabel('PIB Previsto (em escala original)')
axs[1, 1].set_ylabel('PIB Real (em escala original)')

plt.tight_layout()
plt.show()

# Mostrar Tabelas Bonitas

from IPython.display import display

print("\n=== Tabela de VIF ===")
display(vif_table.style.background_gradient(cmap='OrRd'))

print("\n=== Tabela de Coeficientes ===")
display(summary_table.style.background_gradient(cmap='Blues'))

"""---

## Exporta√ß√µes em fun√ß√£o da produ√ß√£o agropecu√°ria e empresas

A an√°lise a seguir tem como objetivo investigar os principais determinantes econ√¥micos das exporta√ß√µes municipais brasileiras por meio da constru√ß√£o de um modelo de regress√£o linear m√∫ltipla. Para isso, foi necess√°rio realizar um processo criterioso de prepara√ß√£o dos dados, que incluiu a sele√ß√£o de vari√°veis explicativas relevantes ‚Äî como indicadores de emprego, produ√ß√£o agropecu√°ria, receitas p√∫blicas e movimenta√ß√µes comerciais ‚Äî e a aplica√ß√£o de transforma√ß√µes logar√≠tmicas. Tais transforma√ß√µes visam corrigir assimetrias e diferen√ßas de escala entre as vari√°veis, al√©m de facilitar a interpreta√ß√£o dos coeficientes em termos de elasticidades.

Antes do ajuste do modelo, foi conduzida uma verifica√ß√£o da multicolinearidade entre as vari√°veis explicativas utilizando o Fator de Infla√ß√£o da Vari√¢ncia (VIF). Esse diagn√≥stico √© essencial para garantir a estabilidade dos coeficientes estimados e a validade das infer√™ncias estat√≠sticas. Vari√°veis com VIF elevado podem indicar redund√¢ncia de informa√ß√£o e comprometer a confiabilidade do modelo, por isso esse passo foi adotado como crit√©rio de valida√ß√£o preliminar.

O modelo de regress√£o foi ajustado utilizando erros padr√£o robustos do tipo HC3, uma abordagem adequada para lidar com heterocedasticidade residual e evitar vieses na estima√ß√£o dos erros padr√£o dos coeficientes. Em seguida, o modelo foi avaliado por meio de gr√°ficos diagn√≥sticos, como res√≠duos versus valores ajustados, QQ-plot, distribui√ß√£o dos res√≠duos e gr√°ficos de influ√™ncia. Essas visualiza√ß√µes permitem verificar a adequa√ß√£o dos pressupostos do modelo, como normalidade e homocedasticidade dos res√≠duos, al√©m de identificar observa√ß√µes potencialmente influentes que possam distorcer os resultados.

Por fim, foi realizada a compara√ß√£o entre os valores reais e previstos de exporta√ß√µes em sua escala original, a fim de validar a capacidade preditiva do modelo. Essa etapa √© fundamental para avaliar se as vari√°veis econ√¥micas selecionadas realmente contribuem para explicar o comportamento das exporta√ß√µes municipais. O modelo, portanto, n√£o apenas fornece uma base quantitativa para compreens√£o do fen√¥meno exportador, como tamb√©m oferece subs√≠dios para interpreta√ß√µes mais amplas sobre o papel de vari√°veis estruturais na din√¢mica econ√¥mica local.
"""

def check_vif(X):
    """Calcula VIF para identificar multicolinearidade"""
    vif_data = pd.DataFrame()
    vif_data["Vari√°vel"] = X.columns
    vif_data["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]
    return vif_data.sort_values("VIF", ascending=False)


def prepare_data(df):
    """Prepara os dados aplicando transforma√ß√£o log e removendo outliers"""
    df = df.copy()

    features = ['PessoalOcupado', 'VrSalarios', 'QtEmpresas',
                'VlProducaoAgricola', 'VlProducaoPecuaria',
                'Importacoes_US$', 'Receitas_R$', 'Transferencias_correntes_R$',
                'Transferencias_capital_R$']

    for col in features + ['Exportacoes_US$']:
        df[f'log_{col}'] = np.log1p(pd.to_numeric(df[col], errors='coerce'))

    df = df.replace([np.inf, -np.inf], np.nan).dropna()

    return df


# Prepara√ß√£o dos Dados

df_prepared = prepare_data(df_br)

# Sele√ß√£o de vari√°veis
X = df_prepared[[f'log_{col}' for col in [
    'PessoalOcupado', 'VrSalarios', 'QtEmpresas',
    'VlProducaoAgricola', 'VlProducaoPecuaria',
    'Importacoes_US$', 'Receitas_R$', 'Transferencias_correntes_R$', 'Transferencias_capital_R$']]]

y = df_prepared['log_Exportacoes_US$']


# Verificar Multicolinearidade

vif_table = check_vif(X)
print("\n=== An√°lise de Multicolinearidade (VIF) ===")
print(vif_table)


# Ajuste do Modelo

X_const = sm.add_constant(X)
modelo_exp = sm.OLS(y, X_const).fit(cov_type='HC3')


# Resultados do Modelo

summary_table = modelo_exp.summary2().tables[1]
print("\n=== Resumo do Modelo de Exporta√ß√µes ===")
print(summary_table)

# Gr√°ficos de Diagn√≥stico

fig, axs = plt.subplots(2, 2, figsize=(12, 8))

# Res√≠duos vs Ajustados
sns.scatterplot(x=modelo_exp.fittedvalues, y=modelo_exp.resid, ax=axs[0, 0])
axs[0, 0].axhline(y=0, color='red', linestyle='--')
axs[0, 0].set_title('Res√≠duos vs Ajustados')
axs[0, 0].set_xlabel('Valores Ajustados')
axs[0, 0].set_ylabel('Res√≠duos')

# QQ Plot
stats.probplot(modelo_exp.resid, dist="norm", plot=axs[0, 1])
axs[0, 1].set_title('QQ-Plot dos Res√≠duos')

# Distribui√ß√£o dos Res√≠duos
sns.histplot(modelo_exp.resid, kde=True, ax=axs[1, 0])
axs[1, 0].set_title('Distribui√ß√£o dos Res√≠duos')
axs[1, 0].set_xlabel('Res√≠duos')

# Exporta√ß√µes Reais vs Previstas
sns.regplot(
    x=np.expm1(modelo_exp.predict(X_const)),
    y=np.expm1(y),
    line_kws={'color': 'red'},
    scatter_kws={'alpha': 0.3},
    ax=axs[1, 1]
)
axs[1, 1].plot(
    [np.expm1(y).min(), np.expm1(y).max()],
    [np.expm1(y).min(), np.expm1(y).max()],
    'k--', label='Perfeita previs√£o'
)
axs[1, 1].set_title('Exporta√ß√µes Reais vs Previstas')
axs[1, 1].set_xlabel('Exporta√ß√µes Previstas (esc. original)')
axs[1, 1].set_ylabel('Exporta√ß√µes Reais (esc. original)')

plt.tight_layout()
plt.show()

# Tabelas

from IPython.display import display

print("\n=== Tabela de VIF ===")
display(vif_table.style.background_gradient(cmap='OrRd'))

print("\n=== Tabela de Coeficientes ===")
display(summary_table.style.background_gradient(cmap='Blues'))

"""---

## Sal√°rios em fun√ß√£o de caracter√≠sticas municipais

A an√°lise tem como objetivo investigar os principais fatores associados √† varia√ß√£o da massa salarial nos munic√≠pios brasileiros, utilizando um modelo de regress√£o linear m√∫ltipla.
O estudo parte de um processo de prepara√ß√£o dos dados, que incluiu a sele√ß√£o de vari√°veis econ√¥micas fundamentais, como PIB, grau de formaliza√ß√£o do mercado de trabalho e intera√ß√µes entre essas vari√°veis, al√©m da aplica√ß√£o de transforma√ß√µes logar√≠tmicas para corre√ß√£o de assimetrias e normaliza√ß√£o de escalas.

 A verifica√ß√£o preliminar de multicolinearidade atrav√©s do Fator de Infla√ß√£o da Vari√¢ncia ,VIF, revelou valores superiores a 100 para todas as vari√°veis inclu√≠das, particularmente para o termo de intera√ß√£o entre PIB e formaliza√ß√£o ,VIF = 265,45. Apesar disso, o modelo alcan√ßou um excelente poder explicativo, com R¬≤ de 0,8997, indicando que aproximadamente 90% da varia√ß√£o nos logaritmos dos sal√°rios municipais √© explicada pelas vari√°veis selecionadas.

O erro padr√£o da estimativa ,RMSE = 0,4812 na escala logar√≠tmica, corresponde a uma margem de aproximadamente 61% na escala original de valores salariais. Os coeficientes estimados revelam essas rela√ß√µes econ√¥micas: o PIB municipal apresenta uma elasticidade unit√°ria, ou seja, um aumento de 1% no PIB est√° associado a um incremento de aproximadamente 1,21% na massa salarial, confirmando a import√¢ncia do desenvolvimento econ√¥mico local para a gera√ß√£o de renda. O grau de formaliza√ß√£o apresenta coeficiente negativo (-0,65), possivelmente refletindo a concentra√ß√£o de trabalhadores formais em cargos com menores sal√°rios em alguns contextos municipais. J√° o termo de intera√ß√£o positivo (0,28) sugere que o efeito do PIB sobre os sal√°rios √© amplificado em munic√≠pios com maior formaliza√ß√£o, indicando sinergia entre desenvolvimento econ√¥mico e estrutura√ß√£o do mercado de trabalho.

A decomposi√ß√£o por regi√µes revela padr√µes distintos: Sul e Sudeste apresentam os melhores ajustes ,R¬≤ superiores a 0,91, com estruturas de coeficientes mais est√°veis; o Nordeste mostra comportamento intermedi√°rio, mantendo a rela√ß√£o positiva entre PIB e sal√°rios; e Norte e Centro-Oeste exibem padr√µes at√≠picos, com coeficientes negativos para o PIB e altos valores absolutos para a formaliza√ß√£o, possivelmente refletindo din√¢micas espec√≠ficas de mercados de trabalho menos consolidados.

Os gr√°ficos de diagn√≥stico complementam a an√°lise num√©rica: a an√°lise de res√≠duos indica dispers√£o homog√™nea em torno de zero nos res√≠duos versus valores preditos ,em escala logar√≠tmica, sugerindo adequa√ß√£o do modelo, sem padr√µes sistem√°ticos de heterocedasticidade; e a compara√ß√£o entre previs√£o e valores reais, com o alinhamento dos pontos em torno da linha de perfeita previs√£o no gr√°fico em escala original, valida a capacidade preditiva do modelo, com maior dispers√£o nos valores mais altos, conforme esperado em an√°lises econ√¥micas municipais.
"""

def prepare_salary_data(df):
    """Prepara os dados para an√°lise de sal√°rios com transforma√ß√µes e sele√ß√£o de vari√°veis"""
    df = df.copy()

    # Transforma√ß√µes logar√≠tmicas
    df['log_VrSalarios'] = np.log1p(df['VrSalarios'])
    df['log_Populacao'] = np.log1p(df['Populacao'])
    df['log_PIB'] = np.log1p(df['PIB'])
    df['log_QtEmpresas'] = np.log1p(df['QtEmpresas'])

    # Criar vari√°veis adicionais que podem ser relevantes
    df['Perc_Assalariados'] = df['PessoalAssalariado'] / df['PessoalOcupado']
    df['Salario_Medio'] = df['VrSalarios'] / df['PessoalAssalariado']

    # Criar intera√ß√£o entre PIB e percentual de assalariados
    df['interaction_PIB_Assal'] = df['log_PIB'] * df['Perc_Assalariados']

    # Remover infinitos e NaNs
    df = df.replace([np.inf, -np.inf], np.nan).dropna()

    return df

# Preparar os dados
df_salarios = prepare_salary_data(df_br)

# Selecionar vari√°veis reduzidas para evitar multicolinearidade
X = df_salarios[['log_PIB', 'Perc_Assalariados', 'interaction_PIB_Assal']]
y = df_salarios['log_VrSalarios']

# Verificar multicolinearidade
def check_vif(X):
    vif_data = pd.DataFrame()
    vif_data["Variable"] = X.columns
    vif_data["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]
    return vif_data

print("=== An√°lise de Multicolinearidade ===")
print(check_vif(X))

# Dividir em treino e teste estratificado por regi√£o
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    random_state=42,
    stratify=df_salarios['Regiao']
)

# Modelo de regress√£o linear
modelo_salarios = LinearRegression()
modelo_salarios.fit(X_train, y_train)

# Previs√µes
y_pred = modelo_salarios.predict(X_test)

# M√©tricas de avalia√ß√£o
print("\n=== M√©tricas do Modelo ===")
print(f"R¬≤: {r2_score(y_test, y_pred):.4f}")
print(f"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred)):.4f}")

# Coeficientes com interpreta√ß√£o percentual
coef_df = pd.DataFrame({
    'Vari√°vel': X.columns,
    'Coef_original': modelo_salarios.coef_,
    'Impacto_percentual': (np.exp(modelo_salarios.coef_) - 1)
})
print("\n=== Coeficientes do Modelo ===")
print(coef_df)

# Gr√°ficos organizados em grid

fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# Gr√°fico de res√≠duos
residuos = y_test - y_pred
sns.scatterplot(ax=axes[0], x=y_pred, y=residuos)
axes[0].axhline(y=0, color='r', linestyle='--')
axes[0].set_xlabel('Valores Previstos (log)')
axes[0].set_ylabel('Res√≠duos')
axes[0].set_title('An√°lise de Res√≠duos')

# Gr√°fico de previs√£o vs real em escala original (R$)
sns.scatterplot(ax=axes[1], x=np.expm1(y_pred), y=np.expm1(y_test), alpha=0.5)
axes[1].plot([np.expm1(y_test).min(), np.expm1(y_test).max()],
              [np.expm1(y_test).min(), np.expm1(y_test).max()],
              'r--')
axes[1].set_xlabel('Sal√°rios Previstos (R$)')
axes[1].set_ylabel('Sal√°rios Reais (R$)')
axes[1].set_title('Sal√°rios Reais vs Previstos (em R$)')

plt.tight_layout()
plt.show()

# An√°lise por regi√£o
regioes = df_salarios['Regiao'].unique()
results = []

for regiao in regioes:
    mask = df_salarios['Regiao'] == regiao
    X_reg = X[mask]
    y_reg = y[mask]

    if len(X_reg) > 10:  # Apenas para regi√µes com dados suficientes
        modelo_reg = LinearRegression().fit(X_reg, y_reg)
        y_pred_reg = modelo_reg.predict(X_reg)

        results.append({
            'Regi√£o': regiao,
            'R¬≤': r2_score(y_reg, y_pred_reg),
            'Coef_PIB': modelo_reg.coef_[0],
            'Coef_Formal': modelo_reg.coef_[1],
            'Coef_Intera√ß√£o': modelo_reg.coef_[2],
            'N_Observa√ß√µes': len(X_reg)
        })

results_df = pd.DataFrame(results)
print("\n=== An√°lise por Regi√£o ===")
print(results_df.sort_values('R¬≤', ascending=False))

"""---

## √ìbitos Infantis em fun√ß√£o de condi√ß√µes socioecon√¥micas

Neste segmento da an√°lise, o foco foi construir um modelo para investigar os fatores associados √† taxa de √≥bitos infantis nos munic√≠pios brasileiros. A vari√°vel dependente utilizada foi a Taxa de √ìbitos Infantis, definida como o n√∫mero de √≥bitos infantis a cada mil nascimentos. Para garantir a integridade da m√©trica, foram tratados os casos em que o n√∫mero de nascimentos era zero, atribuindo NaN para evitar divis√µes por zero.

As vari√°veis explicativas selecionadas refletem aspectos populacionais, econ√¥micos e de infraestrutura local: Popula√ß√£o total, PIB, Valor total dos sal√°rios, Receitas municipais e uma vari√°vel categ√≥rica chamada povoamento, que distingue munic√≠pios urbanizados de √°reas mais rurais.

Antes da modelagem, foi realizada uma etapa de limpeza para remover observa√ß√µes com valores ausentes ou infinitos, assegurando que o modelo fosse ajustado apenas com dados v√°lidos. Com os dados limpos, foi estimado um modelo de regress√£o linear utilizando a biblioteca statsmodels, permitindo uma an√°lise detalhada dos coeficientes, n√≠veis de signific√¢ncia estat√≠stica e medidas de ajuste.

Por fim, foi gerado um gr√°fico de res√≠duos com suaviza√ß√£o lowess, que permite visualizar poss√≠veis padr√µes n√£o explicados pelo modelo. Essa an√°lise gr√°fica √© essencial para avaliar a qualidade do ajuste e identificar viola√ß√µes de pressupostos, como n√£o linearidade ou heterocedasticidade.
"""

# Criar taxa de √≥bitos infantis
# Certificar-se de que NrNascimentos n√£o √© zero para evitar divis√£o por zero
df_br['TaxaObitosInfantis'] = np.where(
    df_br['NrNascimentos'] != 0,
    df_br['NrObitosInfantis'] / df_br['NrNascimentos'] * 1000,
    np.nan # Coloca NaN onde a divis√£o seria por zero
)

X_cols = ['Populacao', 'PIB', 'VrSalarios', 'Receitas_R$', 'povoamento']
y_col = 'TaxaObitosInfantis'

# Selecionar as colunas relevantes
df_analysis = df_br[X_cols + [y_col]].copy()

# Remover linhas com NaN ou infinito nas colunas selecionadas para a an√°lise
df_analysis = df_analysis.replace([np.inf, -np.inf], np.nan).dropna()

# Verificar se ainda h√° dados ap√≥s a limpeza
if df_analysis.empty:
    print("Ap√≥s a limpeza, n√£o restam dados para a an√°lise. Verifique os dados originais.")
else:
    X = df_analysis[X_cols]
    y = df_analysis[y_col]

    # Modelo com statsmodels para an√°lise detalhada
    X = sm.add_constant(X)
    modelo_obitos = sm.OLS(y, X).fit()

    print(modelo_obitos.summary())

    # Visualizar res√≠duos
    plt.figure(figsize=(10,6))
    # Certificar-se de que os dados para o scatterplot n√£o cont√™m NaNs ou infinitos
    predictions = modelo_obitos.predict(X)
    sns.residplot(x=predictions, y=y, lowess=True, line_kws={'color': 'red'})
    plt.xlabel('Valores Previstos')
    plt.ylabel('Res√≠duos')
    plt.title('An√°lise de Res√≠duos')
    plt.show()

"""---

## N√∫mero de empresas em fun√ß√£o do PIB e sal√°rios

Antes de ajustar o modelo de regress√£o linear, foi realizada uma etapa cuidadosa de prepara√ß√£o dos dados. Inicialmente, foram selecionadas vari√°veis econ√¥micas relevantes para explicar o n√∫mero de empresas em um munic√≠pio, sendo elas: PIB, valor total de sal√°rios, n√∫mero de pessoas ocupadas e receitas p√∫blicas. Como essas vari√°veis vieram do arquivo em formato texto (strings), foi necess√°rio aplicar um processo de limpeza para transform√°-las em valores num√©ricos v√°lidos, eliminando pontos, v√≠rgulas e outros s√≠mbolos. Ap√≥s essa etapa, as vari√°veis foram transformadas com a fun√ß√£o logar√≠tmica log1p para lidar com escalas assim√©tricas e normalizar a distribui√ß√£o dos dados, o que √© uma pr√°tica comum para melhorar a qualidade dos ajustes em modelos lineares.

Em seguida, avaliou-se a presen√ßa de multicolinearidade entre as vari√°veis independentes utilizando o Fator de Infla√ß√£o da Vari√¢ncia (VIF), uma m√©trica que identifica redund√¢ncias entre os preditores e poss√≠veis distor√ß√µes nos coeficientes estimados. Ap√≥s garantir a viabilidade das vari√°veis, foi ajustado um modelo de regress√£o linear m√∫ltipla, utilizando erros robustos (HC3) para controlar poss√≠veis heterocedasticidades nos res√≠duos.

Por fim, foram gerados tr√™s gr√°ficos fundamentais para o diagn√≥stico do modelo: o gr√°fico de res√≠duos versus valores ajustados, que permite verificar a presen√ßa de padr√µes sistem√°ticos (indicando viola√ß√£o da suposi√ß√£o de homocedasticidade), o QQ-plot dos res√≠duos, que avalia a ader√™ncia √† normalidade, e a compara√ß√£o entre os valores reais e previstos do n√∫mero de empresas, em escala original, permitindo observar a qualidade preditiva do modelo de maneira mais intuitiva.
"""

cols_to_clean = ['PIB', 'VrSalarios', 'PessoalOcupado', 'Receitas_R$', 'QtEmpresas']

# --- Fun√ß√£o para calcular VIF ---
def check_vif(X):
    vif_data = pd.DataFrame()
    vif_data["Variable"] = X.columns
    vif_data["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]
    return vif_data.sort_values("VIF", ascending=False)

# --- Sele√ß√£o das vari√°veis log-transformadas ---
features = ['log_PIB', 'log_VrSalarios', 'log_PessoalOcupado', 'log_Receitas_R$']
target = 'log_QtEmpresas'

X = df_br_clean[features]
y = df_br_clean[target]

# Verificar multicolinearidade
print("=== An√°lise de Multicolinearidade (VIF) ===")
print(check_vif(X))

# Adiciona intercepto para regress√£o
X = sm.add_constant(X)

# Ajustar modelo com erros robustos
modelo = sm.OLS(y, X).fit(cov_type='HC3')

print("\n=== Sum√°rio do Modelo ===")
print(modelo.summary())

# --- Diagn√≥stico gr√°fico ---

fig, axs = plt.subplots(2, 2, figsize=(14, 10))

# Res√≠duos vs Ajustados
sns.scatterplot(x=modelo.fittedvalues, y=modelo.resid, alpha=0.5, ax=axs[0, 0])
axs[0, 0].axhline(0, color='red', linestyle='--')
axs[0, 0].set_xlabel('Valores Ajustados')
axs[0, 0].set_ylabel('Res√≠duos')
axs[0, 0].set_title('Res√≠duos vs Valores Ajustados')

# QQ-plot dos res√≠duos
stats.probplot(modelo.resid, dist="norm", plot=axs[0, 1])
axs[0, 1].set_title('QQ-Plot dos Res√≠duos')

# Distribui√ß√£o dos res√≠duos
sns.histplot(modelo.resid, kde=True, color='green', ax=axs[1, 0])
axs[1, 0].set_title('Distribui√ß√£o dos Res√≠duos')
axs[1, 0].set_xlabel('Res√≠duos')

# Real vs Previsto (escala original)
sns.scatterplot(
    x=np.expm1(modelo.predict(X)),
    y=np.expm1(y),
    alpha=0.3,
    ax=axs[1, 1]
)
axs[1, 1].plot(
    [np.expm1(y).min(), np.expm1(y).max()],
    [np.expm1(y).min(), np.expm1(y).max()],
    'k--', label='Perfeita previs√£o'
)
axs[1, 1].set_xlabel('N√∫mero de Empresas Previsto')
axs[1, 1].set_ylabel('N√∫mero de Empresas Real')
axs[1, 1].set_title('N√∫mero de Empresas Real vs Previsto')

plt.tight_layout()
plt.show()

"""---

##An√°lise da Rela√ß√£o entre Popula√ß√£o e N√∫mero de Nascimentos nas Cidades Brasileiras por Meio de Regress√£o Linear

A presente an√°lise tem como objetivo investigar se existe uma rela√ß√£o linear entre a popula√ß√£o de um munic√≠pio e o n√∫mero de nascimentos registrados. Intuitivamente, espera-se que cidades com maior popula√ß√£o apresentem tamb√©m um maior n√∫mero de nascimentos, dado que h√° mais indiv√≠duos em idade f√©rtil.

Para isso, utilizaremos a t√©cnica de Regress√£o Linear Simples, que permite modelar a rela√ß√£o entre uma vari√°vel independente ‚Äî no caso, a Popula√ß√£o ‚Äî e uma vari√°vel dependente ‚Äî o N√∫mero de Nascimentos. Atrav√©s deste modelo, poderemos responder quest√µes como:

O n√∫mero de nascimentos cresce proporcionalmente com a popula√ß√£o?

Existem cidades que fogem deste padr√£o?

O modelo √© suficientemente bom para fazer previs√µes?

Al√©m disso, a an√°lise do coeficiente de determina√ß√£o (R¬≤) nos permitir√° avaliar o qu√£o bem a popula√ß√£o explica as varia√ß√µes no n√∫mero de nascimentos.
"""

from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score

# Verificando os dados relevantes e removendo NaNs
df_br = df_br[['UF', 'Populacao', 'NrNascimentos']].dropna()

# Definindo vari√°veis
X = df_br[['Populacao']]  # Vari√°vel independente
y = df_br['NrNascimentos']  # Vari√°vel dependente

# Criando o modelo de regress√£o linear
model = LinearRegression()
model.fit(X, y)

# Pegando os coeficientes
coeficiente = model.coef_[0]  # Inclina√ß√£o
intercepto = model.intercept_  # Intercepto

# Fazendo previs√µes
y_pred = model.predict(X)

# Avaliando o modelo
r2 = r2_score(y, y_pred)

# Plotando os dados e a linha de regress√£o
plt.figure(figsize=(10,6))
sns.scatterplot(x='Populacao', y='NrNascimentos', data=df_br, color='blue', label='Dados reais')
plt.plot(df_br['Populacao'], y_pred, color='red', label='Regress√£o Linear')
plt.title('Rela√ß√£o entre Popula√ß√£o e N√∫mero de Nascimentos')
plt.xlabel('Popula√ß√£o')
plt.ylabel('N√∫mero de Nascimentos')
plt.legend()
plt.grid(True)
plt.show()

# Exibindo os resultados
print('Equa√ß√£o da reta:')
print(f'NrNascimentos = {coeficiente:.4f} * Populacao + {intercepto:.2f}')
print(f'R¬≤ do modelo: {r2:.4f}')

"""Equa√ß√£o da Reta encontrada foi: NrNascimentos=0.0122√óPopulacao+1677.96

O coeficiente de determina√ß√£o (R¬≤ = 0.669) indica que aproximadamente 66,9% da varia√ß√£o no n√∫mero de nascimentos pode ser explicada pela varia√ß√£o na popula√ß√£o. Isso evidencia uma tend√™ncia clara de que munic√≠pios com maior popula√ß√£o tendem a apresentar maior n√∫mero de nascimentos. Contudo, h√° outros fatores n√£o contemplados por esse modelo que tamb√©m influenciam esse fen√¥meno, como diferen√ßas culturais, econ√¥micas e demogr√°ficas entre as cidades.

<br>O gr√°fico de dispers√£o refor√ßa visualmente essa tend√™ncia, embora a presen√ßa de alguns pontos discrepantes (outliers) sugira que o modelo linear, apesar de robusto, n√£o captura totalmente a complexidade da rela√ß√£o em todas as regi√µes do pa√≠s.
"""

ufs = sorted(df_br['UF'].unique())

# Tamanho do grid
n_ufs = len(ufs)
n_cols = 4
n_rows = int(np.ceil(n_ufs / n_cols))

# Cria figura e eixos
fig, axs = plt.subplots(n_rows, n_cols, figsize=(n_cols*4, n_rows*3))
axs = axs.flatten()  # Facilita o acesso aos subplots com 1D

for i, uf in enumerate(ufs):
    df_uf = df_br[df_br['UF'] == uf]

    X_uf = df_uf[['Populacao']]
    y_uf = df_uf['NrNascimentos']

    ax = axs[i]  # Subplot correspondente

    if len(df_uf) < 5:
        ax.text(0.5, 0.5, f'Dados insuficientes\n({len(df_uf)} mun.)',
                ha='center', va='center', fontsize=8)
        ax.set_title(uf)
        ax.axis('off')
        continue

    model_uf = LinearRegression()
    model_uf.fit(X_uf, y_uf)
    y_pred_uf = model_uf.predict(X_uf)
    r2_uf = r2_score(y_uf, y_pred_uf)

    # Gr√°fico no subplot
    ax.scatter(df_uf['Populacao'], df_uf['NrNascimentos'], color='blue', alpha=0.5, s=10, label='Dados')
    ax.plot(df_uf['Populacao'], y_pred_uf, color='red', linewidth=1, label='Regress√£o')
    ax.set_title(f'{uf} (R¬≤={r2_uf:.2f})', fontsize=9)
    ax.set_xlabel('Popula√ß√£o', fontsize=8)
    ax.set_ylabel('Nascimentos', fontsize=8)
    ax.tick_params(axis='both', labelsize=7)
    ax.grid(True)

# Esconde subplots extras, se houver
for j in range(i+1, len(axs)):
    axs[j].axis('off')

plt.tight_layout()
plt.show()

"""Ap√≥s a visualiza√ß√£o desses dados, utilizarei de alguns estados para an√°lise a seguir:

O coeficiente de determina√ß√£o R¬≤ indica quanto da varia√ß√£o no n√∫mero de nascimentos pode ser explicada pela popula√ß√£o do munic√≠pio. Ele varia entre 0 (nenhuma explica√ß√£o) e 1 (explica√ß√£o perfeita).

* Estados como o Rio de Janeiro (R¬≤ = 0.92) apresentam uma forte rela√ß√£o linear entre popula√ß√£o e nascimentos. Isso sugere que, nos munic√≠pios fluminenses, o n√∫mero de nascimentos cresce proporcionalmente com o tamanho da popula√ß√£o ‚Äî provavelmente por haver uma distribui√ß√£o mais homog√™nea dos servi√ßos de sa√∫de e perfil demogr√°fico.

* J√° no Paran√° (R¬≤ = 0.37), essa rela√ß√£o √© mais fraca, o que pode indicar maior variabilidade em fatores que afetam os nascimentos ‚Äî como estrutura et√°ria, acesso a maternidade, pol√≠ticas p√∫blicas, ou at√© migra√ß√£o interna.

* No Acre (R¬≤ = 0.06), praticamente n√£o h√° rela√ß√£o linear entre popula√ß√£o e n√∫mero de nascimentos. Isso pode ocorrer em estados com pequena popula√ß√£o, grande heterogeneidade entre munic√≠pios, ou dados inconsistentes (ex: subnotifica√ß√£o, dados administrativos irregulares).

<br>A an√°lise mostra que a popula√ß√£o √© um bom preditor de nascimentos em alguns estados, mas n√£o em todos. Portanto, modelos mais robustos ou com m√∫ltiplas vari√°veis (como idade m√©dia, renda, acesso √† sa√∫de) podem ser necess√°rios para explicar melhor essa rela√ß√£o em estados com R¬≤ baixos.

---

##NrNascimentos em fun√ß√£o de QtEmpresas e VlProducaoPecuaria

Nesta etapa da an√°lise, foi investigada a rela√ß√£o entre o n√∫mero de nascimentos em um munic√≠pio e vari√°veis econ√¥micas locais. A vari√°vel dependente utilizada foi o n√∫mero de nascimentos (NrNascimentos), e as vari√°veis explicativas escolhidas foram o n√∫mero de empresas registradas (QtEmpresas) e o valor da produ√ß√£o pecu√°ria (VlProducaoPecuaria), que representam aspectos da atividade econ√¥mica municipal. Ap√≥s a sele√ß√£o das colunas relevantes, foram removidas as observa√ß√µes com dados ausentes para garantir a consist√™ncia da an√°lise. Em seguida, foi ajustado um modelo de regress√£o linear com intercepto, utilizando a biblioteca statsmodels. O modelo fornece estimativas dos coeficientes e medidas de ajuste, permitindo avaliar a influ√™ncia das vari√°veis econ√¥micas sobre os nascimentos. Para diagnosticar poss√≠veis desvios dos pressupostos do modelo, foi gerado um gr√°fico de res√≠duos suavizado, √∫til para identificar padr√µes de heterocedasticidade ou n√£o linearidade.
"""

# Selecionar as colunas relevantes
df = df_br[['NrNascimentos', 'QtEmpresas', 'VlProducaoPecuaria']]

# Remover linhas com valores ausentes
df = df.dropna()

# Definir vari√°veis independentes (X) e dependente (y)
X = df[['QtEmpresas', 'VlProducaoPecuaria']]
y = df['NrNascimentos']

# Adicionar constante (intercepto) ao modelo
X = sm.add_constant(X)

# Criar o modelo de regress√£o linear
modelo = sm.OLS(y, X).fit()

# Exibir o resumo dos resultados
print(modelo.summary())

# -------------------------------
# Plotar gr√°fico de res√≠duos
sns.residplot(x=modelo.fittedvalues, y=modelo.resid, lowess=True, line_kws={'color': 'red'})
plt.xlabel('Valores ajustados')
plt.ylabel('Res√≠duos')
plt.title('Res√≠duos x Valores Ajustados (SEM PIB)')
plt.show()

"""A partir dos resultados obtidos, √© poss√≠vel concluir que se trata de uma rela√ß√£o n√£o linear.
Com o intuito de aperfei√ßoarmos nossa predi√ß√£o, resolvemos adicionar o PIB como vari√°vel para a mesma. Os resultados podem ser vistos abaixo.
"""

# Selecionar as colunas relevantes
df = df_br[['NrNascimentos', 'QtEmpresas', 'VlProducaoPecuaria', 'PIB']]

# Remover linhas com valores ausentes
df = df.dropna()

# Definir vari√°veis independentes (X) e dependente (y)
X = df[['QtEmpresas', 'VlProducaoPecuaria', 'PIB']]
y = df['NrNascimentos']

# Adicionar constante (intercepto) ao modelo
X = sm.add_constant(X)

# Criar o modelo de regress√£o linear
modelo = sm.OLS(y, X).fit()

# Exibir o resumo dos resultados
print(modelo.summary())

# -------------------------------
# Plot 1 - Res√≠duos vs Valores Ajustados
sns.residplot(x=modelo.fittedvalues, y=modelo.resid,
              lowess=True, line_kws={'color': 'red'})
plt.xlabel('Valores Ajustados')
plt.ylabel('Res√≠duos')
plt.title('Res√≠duos x Valores Ajustados (COM PIB)')
plt.show()

"""Observando o resultado obtido ao incluir o PIB como vari√°vel, √© poss√≠vel perceber que o R-squared teve um leve aumento e a predi√ß√£o se manteve quase que inteiramente na parte positiva do eixo Y e com uma inclina√ß√£o crescente.

O que observamos:

* Existe uma forte concentra√ß√£o de pontos na regi√£o de valores ajustados mais baixos, indicando poss√≠vel aglomera√ß√£o de munic√≠pios com caracter√≠sticas semelhantes.

* A dispers√£o dos res√≠duos n√£o √© completamente aleat√≥ria: h√° um leve padr√£o de aumento na variabilidade dos res√≠duos conforme crescem os valores ajustados.

* A reta vermelha representa a tend√™ncia m√©dia dos res√≠duos e estar inclinada levemente, al√©m de cruzar o eixo Y em regi√µes negativas, indica que o modelo pode estar superestimando para alguns valores e subestimando para outros ‚Äî evid√™ncia de poss√≠vel heterocedasticidade.

Abaixo, temos um gr√°fico de Plot de Res√≠duos, que verifica se os res√≠duos seguem uma distribui√ß√£o normal, o que √© uma das premissas da regress√£o linear.
"""

# Plot 2 - QQ-Plot dos res√≠duos
sm.qqplot(modelo.resid, line='45', fit=True)
plt.title('QQ-Plot dos Res√≠duos')
plt.show()

"""O que observamos:

* Os pontos deveriam se alinhar sobre a linha vermelha (que representa uma distribui√ß√£o perfeitamente normal).

* No entanto, os pontos apresentam curvatura, especialmente nas extremidades (caudas), o que indica que os res√≠duos n√£o seguem uma distribui√ß√£o normal.

* Isso sugere que h√° presen√ßa de outliers ou assimetria, afetando a qualidade estat√≠stica do modelo.

Por fim, temos um gr√°fico da Distribui√ß√£o dos Res√≠duos (Histograma). Este gr√°fico mostra a distribui√ß√£o dos res√≠duos para verificar visualmente sua forma.
"""

# Plot 3 - Distribui√ß√£o dos res√≠duos
sns.histplot(modelo.resid, kde=True, bins=30, color='blue')
plt.xlabel('Res√≠duos')
plt.title('Distribui√ß√£o dos Res√≠duos')
plt.show()

"""O que observamos:

* A distribui√ß√£o est√° claramente fortemente concentrada ao redor de zero, mas com uma cauda longa, tanto √† direita quanto √† esquerda.

* Isso confirma o que foi visto no QQ-Plot: os res√≠duos n√£o seguem uma distribui√ß√£o normal perfeita, possuem assimetria e presen√ßa de valores extremos.

* Este comportamento pode afetar a confiabilidade dos intervalos de confian√ßa e dos testes estat√≠sticos do modelo.

**Conclus√£o Geral da Predi√ß√£o de Natalidade baseada na Quantidade de Empresas, Valor de Produ√ß√£o Pecu√°ria e PIB**

Os tr√™s gr√°ficos indicam que, embora o modelo consiga captar parte da rela√ß√£o entre nascimentos, PIB, produ√ß√£o pecu√°ria e quantidade de empresas, ele apresenta problemas importantes:

* Poss√≠vel heterocedasticidade (vari√¢ncia dos res√≠duos n√£o constante).

* Viola√ß√£o da normalidade dos res√≠duos, indicando que o modelo linear pode n√£o ser o mais adequado para todos os dados.
"""